{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI vs ì¸ê°„ ìƒì„± í…ìŠ¤íŠ¸ ë¶„ë¥˜ê¸° (LLM ê¸°ë°˜ - í•œêµ­ì–´ Colab ìµœì í™”)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ Google Colab ì‹¤í–‰ ê°€ì´ë“œ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa14ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (Colab í™˜ê²½ì— ìµœì í™”)\n",
    "!pip install -q transformers accelerate bitsandbytes peft datasets sentence-transformers\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 # T4 GPU ì§€ì› PyTorch\n",
    "!pip install -q gdown # Google Drive íŒŒì¼ ë‹¤ìš´ë¡œë“œìš©\n",
    "!pip install -q psutil # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ìš©\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import ë° í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import gc\n",
    "import psutil\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from accelerate import Accelerator\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "\n",
    "print(\"âœ… í•„ìš”í•œ ëª¨ë“ˆ ì„í¬íŠ¸ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ GPU ë° ë©”ëª¨ë¦¬ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "    return memory_mb\n",
    "\n",
    "print(f\"ğŸ” ì‹œì‘ ì‹œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {get_memory_usage():.1f}MB\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   - ì´ ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"âš ï¸ GPU ì‚¬ìš© ë¶ˆê°€ - CPU ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"ğŸ§¹ ì´ˆê¸° ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¥ ë°ì´í„° ë¡œë“œ (Google Drive ìë™ ë‹¤ìš´ë¡œë“œ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "\n",
    "# íŒŒì¼ ID ì„¤ì • (Google Drive ê³µê°œ ë§í¬ì—ì„œ ì¶”ì¶œ)\n",
    "# ì‹¤ì œ ëŒ€íšŒ ë°ì´í„°ì…‹ì˜ Google Drive IDë¡œ êµì²´í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "TRAIN_FILE_ID = \"1teA9GmYlIsutaDLWvCCsLeh7833t-TC_\" # ì˜ˆì‹œ ID, ì‹¤ì œ IDë¡œ ë³€ê²½ í•„ìš”\n",
    "TEST_FILE_ID = \"1bGC_YWtNUOroHARfmzCjrL8oPcvb7Tpw\"  # ì˜ˆì‹œ ID, ì‹¤ì œ IDë¡œ ë³€ê²½ í•„ìš”\n",
    "SAMPLE_FILE_ID = \"1ebrHVj-CtM-7aEz4OqP-bCHlazle-PKM\" # ì˜ˆì‹œ ID, ì‹¤ì œ IDë¡œ ë³€ê²½ í•„ìš”\n",
    "\n",
    "def download_from_drive(file_id, filename):\n",
    "    if file_id and not os.path.exists(filename):\n",
    "        url = f'https://drive.google.com/uc?id={file_id}'\n",
    "        print(f\"ğŸ“¥ {filename} ë‹¤ìš´ë¡œë“œ ì¤‘...\")\n",
    "        gdown.download(url, filename, quiet=False)\n",
    "        print(f\"âœ… {filename} ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!\")\n",
    "    elif os.path.exists(filename):\n",
    "        print(f\"âœ… {filename} ì´ë¯¸ ì¡´ì¬í•¨\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ {filename} íŒŒì¼ IDê°€ ì—†ê±°ë‚˜ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ í•„ìš”.\")\n",
    "\n",
    "# ìë™ ë‹¤ìš´ë¡œë“œ ì‹¤í–‰\n",
    "print(\"ğŸ“ ë°ì´í„° íŒŒì¼ ë¡œë”© ì‹œì‘...\")\n",
    "download_from_drive(TRAIN_FILE_ID, 'train.csv')\n",
    "download_from_drive(TEST_FILE_ID, 'test.csv')\n",
    "download_from_drive(SAMPLE_FILE_ID, 'sample_submission.csv')\n",
    "\n",
    "# CSV íŒŒì¼ ì½ê¸°\n",
    "try:\n",
    "    train_df = pd.read_csv('train.csv', encoding='utf-8-sig')\n",
    "    test_df = pd.read_csv('test.csv', encoding='utf-8-sig')\n",
    "    sample_submission_df = pd.read_csv('sample_submission.csv', encoding='utf-8-sig')\n",
    "    print(\"âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ!\")\n",
    "    print(f\"   - Train shape: {train_df.shape}\")\n",
    "    print(f\"   - Test shape: {test_df.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    print(\"   - íŒŒì¼ì´ ì˜¬ë°”ë¥´ê²Œ ë‹¤ìš´ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
    "    print(\"   - ì¸ì½”ë”© ë¬¸ì œì¼ ìˆ˜ ìˆìœ¼ë‹ˆ, ë‹¤ë¥¸ ì¸ì½”ë”©ì„ ì‹œë„í•´ë³´ì„¸ìš” (ì˜ˆ: cp949, euc-kr).\")\n",
    "    raise\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"ğŸ§¹ ë°ì´í„° ë¡œë“œ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š ë°ì´í„° ì „ì²˜ë¦¬ ë° í† í°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'paragraph_text' ì»¬ëŸ¼ì„ 'full_text'ë¡œ í†µì¼ (í•„ìš”í•œ ê²½ìš°)\n",
    "if 'paragraph_text' in test_df.columns and 'full_text' not in test_df.columns:\n",
    "    test_df = test_df.rename(columns={'paragraph_text': 'full_text'})\n",
    "    print(\"âœ… Test ë°ì´í„°ì˜ 'paragraph_text' ì»¬ëŸ¼ì„ 'full_text'ë¡œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# í›ˆë ¨ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„° ë¶„í• \n",
    "X = train_df[['title', 'full_text']]\n",
    "y = train_df['generated']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.05, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"âœ… í›ˆë ¨ ì„¸íŠ¸: {X_train.shape}, ê²€ì¦ ì„¸íŠ¸: {X_val.shape}\")\n",
    "print(f\"   - í›ˆë ¨ ì„¸íŠ¸ AI ë¹„ìœ¨: {y_train.mean():.3f}\")\n",
    "print(f\"   - ê²€ì¦ ì„¸íŠ¸ AI ë¹„ìœ¨: {y_val.mean():.3f}\")\n",
    "\n",
    "# NumPy 2.0 í˜¸í™˜ - PyTorch Dataset í´ë˜ìŠ¤ ì§ì ‘ êµ¬í˜„\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "\n",
    "class TextDataset(TorchDataset):\n",
    "    def __init__(self, texts, labels=None):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {'text': str(self.texts[idx])}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = int(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ (NumPy ë°°ì—´ ì‚¬ìš© ì•ˆí•¨)\n",
    "train_texts = [str(title) + ' ' + str(text) for title, text in zip(X_train['title'], X_train['full_text'])]\n",
    "val_texts = [str(title) + ' ' + str(text) for title, text in zip(X_val['title'], X_val['full_text'])]\n",
    "test_texts = [str(title) + ' ' + str(text) for title, text in zip(test_df['title'], test_df['full_text'])]\n",
    "\n",
    "# ë¼ë²¨ ì¤€ë¹„ (NumPy ë°°ì—´ ì‚¬ìš© ì•ˆí•¨)\n",
    "train_labels = y_train.tolist()\n",
    "val_labels = y_val.tolist()\n",
    "\n",
    "# PyTorch Dataset ìƒì„±\n",
    "torch_train_dataset = TextDataset(train_texts, train_labels)\n",
    "torch_val_dataset = TextDataset(val_texts, val_labels)\n",
    "torch_test_dataset = TextDataset(test_texts)\n",
    "\n",
    "print(\"âœ… PyTorch Dataset ìƒì„± ì™„ë£Œ!\")\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ë¡œë“œ (í•œêµ­ì–´ ëª¨ë¸)\n",
    "MODEL_NAME = \"klue/roberta-base\" # í•œêµ­ì–´ RoBERTa ëª¨ë¸\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Hugging Face Datasetìœ¼ë¡œ ë³€í™˜ (NumPy 2.0 í˜¸í™˜)\n",
    "def convert_to_hf_dataset(torch_dataset):\n",
    "    # ëª¨ë“  ë°ì´í„°ë¥¼ í•œ ë²ˆì— ë³€í™˜\n",
    "    data_dict = {'text': []}\n",
    "    if hasattr(torch_dataset, 'labels') and torch_dataset.labels is not None:\n",
    "        data_dict['labels'] = []\n",
    "    \n",
    "    for i in range(len(torch_dataset)):\n",
    "        item = torch_dataset[i]\n",
    "        data_dict['text'].append(item['text'])\n",
    "        if 'labels' in item:\n",
    "            data_dict['labels'].append(item['labels'])\n",
    "    \n",
    "    return Dataset.from_dict(data_dict)\n",
    "\n",
    "train_dataset = convert_to_hf_dataset(torch_train_dataset)\n",
    "val_dataset = convert_to_hf_dataset(torch_val_dataset)\n",
    "test_dataset = convert_to_hf_dataset(torch_test_dataset)\n",
    "\n",
    "# í† í°í™” í•¨ìˆ˜\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=512, padding=False)\n",
    "\n",
    "print(\"â³ ë°ì´í„° í† í°í™” ì¤‘...\")\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# PyTorch í˜•ì‹ìœ¼ë¡œ ì„¤ì •\n",
    "tokenized_train_dataset.set_format(\"torch\")\n",
    "tokenized_val_dataset.set_format(\"torch\")\n",
    "tokenized_test_dataset.set_format(\"torch\")\n",
    "\n",
    "print(\"âœ… ë°ì´í„° í† í°í™” ë° í˜•ì‹ ì„¤ì • ì™„ë£Œ!\")\n",
    "print(f\"   - í›ˆë ¨ ë°ì´í„°ì…‹: {len(tokenized_train_dataset)} ìƒ˜í”Œ\")\n",
    "print(f\"   - ê²€ì¦ ë°ì´í„°ì…‹: {len(tokenized_val_dataset)} ìƒ˜í”Œ\")\n",
    "print(f\"   - í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹: {len(tokenized_test_dataset)} ìƒ˜í”Œ\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"ğŸ§¹ í† í°í™” í›„ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§  ëª¨ë¸ ë¡œë“œ ë° PEFT (LoRA) ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4ë¹„íŠ¸ ì–‘ìí™” ì„¤ì • (ë©”ëª¨ë¦¬ ì ˆì•½)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 # T4 GPUëŠ” bfloat16 ì§€ì›\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ (Sequence Classificationìš©)\n",
    "print(f\"â³ {MODEL_NAME} ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=1, # ì´ì§„ ë¶„ë¥˜ (0 ë˜ëŠ” 1)\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16, # ëª¨ë¸ dtype ì„¤ì •\n",
    ")\n",
    "\n",
    "print(\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
    "\n",
    "# LoRA ì„¤ì • (PEFT)\n",
    "lora_config = LoraConfig(\n",
    "    r=16, # LoRA ë­í¬ (llm-detect-aiì˜ v26 ì„¤ì • ì°¸ê³ )\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    target_modules=[\"query\", \"key\", \"value\"] # RoBERTa ëª¨ë¸ì˜ ì–´í…ì…˜ ë ˆì´ì–´\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"âœ… LoRA ì„¤ì • ì™„ë£Œ!\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"ğŸ§¹ ëª¨ë¸ ë¡œë“œ ë° LoRA ì„¤ì • í›„ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í›ˆë ¨ íŒŒë¼ë¯¸í„° ë° DataLoader ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# ë°ì´í„° ì½œë ˆì´í„° (ë™ì  íŒ¨ë”©)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# DataLoader ì„¤ì •\n",
    "BATCH_SIZE = 8 # T4 GPUì— ì í•©í•œ ë°°ì¹˜ ì‚¬ì´ì¦ˆ, í•„ìš”ì‹œ ì¡°ì ˆ\n",
    "GRADIENT_ACCUMULATION_STEPS = 4 # ê²½ì‚¬ ëˆ„ì  ë‹¨ê³„ (ì‹¤ì œ ë°°ì¹˜ ì‚¬ì´ì¦ˆ = BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS)\n",
    "NUM_EPOCHS = 3 # í›ˆë ¨ ì—í¬í¬ ìˆ˜\n",
    "LEARNING_RATE = 2e-5 # í•™ìŠµë¥ \n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_val_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    tokenized_test_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "print(\"âœ… DataLoader ì„¤ì • ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í›ˆë ¨ ë£¨í”„ (Accelerate í™œìš©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "accelerator = Accelerator(gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "model, optimizer, train_dataloader, eval_dataloader, test_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader, test_dataloader\n",
    ")\n",
    "\n",
    "num_training_steps = NUM_EPOCHS * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"cosine\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=int(num_training_steps * 0.1),\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "print(\"ğŸš€ í›ˆë ¨ ì‹œì‘!\")\n",
    "progress_bar = tqdm(range(num_training_steps), disable=not accelerator.is_local_main_process)\n",
    "\n",
    "model.train()\n",
    "best_auc = -1.0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        with accelerator.accumulate(model):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if accelerator.sync_gradients:\n",
    "            progress_bar.update(1)\n",
    "            if (step + 1) % 100 == 0: # 100 ìŠ¤í…ë§ˆë‹¤ ë¡œê¹…\n",
    "                accelerator.print(f\"Epoch {epoch+1}, Step {step+1}/{len(train_dataloader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # ì—í¬í¬ ì¢…ë£Œ í›„ ê²€ì¦\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for batch in eval_dataloader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        predictions = torch.sigmoid(outputs.logits).squeeze().cpu().numpy()\n",
    "        labels = batch[\"labels\"] .cpu().numpy()\n",
    "        all_preds.extend(predictions)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "    auc_score = roc_auc_score(all_labels, all_preds)\n",
    "    accelerator.print(f\"Epoch {epoch+1} ê²€ì¦ AUC: {auc_score:.4f}\")\n",
    "\n",
    "    if auc_score > best_auc:\n",
    "        best_auc = auc_score\n",
    "        accelerator.save_state(\"./best_model_checkpoint\")\n",
    "        accelerator.print(f\"ğŸ‰ ìƒˆë¡œìš´ ìµœê³  AUC ë‹¬ì„±! ëª¨ë¸ ì €ì¥ë¨: {best_auc:.4f}\")\n",
    "\n",
    "    model.train()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "accelerator.print(\"âœ… í›ˆë ¨ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”® ì˜ˆì¸¡ (Inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì  ëª¨ë¸ ë¡œë“œ (í›ˆë ¨ì´ ì™„ë£Œëœ í›„)\n",
    "print(\"â³ ìµœì  ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "accelerator.load_state(\"./best_model_checkpoint\")\n",
    "model.eval()\n",
    "\n",
    "all_test_preds = []\n",
    "print(\"ğŸ”® í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ì¤‘...\")\n",
    "for batch in tqdm(test_dataloader, disable=not accelerator.is_local_main_process):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    predictions = torch.sigmoid(outputs.logits).squeeze().cpu().numpy()\n",
    "    all_test_preds.extend(predictions)\n",
    "\n",
    "probs = np.array(all_test_preds)\n",
    "\n",
    "print(\"âœ… ì˜ˆì¸¡ ì™„ë£Œ!\")\n",
    "print(f\"   - ì˜ˆì¸¡ê°’ ê°œìˆ˜: {len(probs)}\")\n",
    "print(f\"   - ì˜ˆì¸¡ê°’ ë²”ìœ„: [{probs.min():.3f}, {probs.max():.3f}]\")\n",
    "print(f\"   - ì˜ˆì¸¡ê°’ í‰ê· : {probs.mean():.3f}\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"ğŸ§¹ ì˜ˆì¸¡ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¤ ì œì¶œ íŒŒì¼ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_submission íŒŒì¼ ì½ê¸° (ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆì„ ìˆ˜ ìˆìŒ)\n",
    "if 'sample_submission_df' not in locals():\n",
    "    sample_submission_df = pd.read_csv('sample_submission.csv', encoding='utf-8-sig')\n",
    "\n",
    "sample_submission_df['generated'] = probs\n",
    "\n",
    "output_filename = 'submission_llm_korean_colab.csv'\n",
    "sample_submission_df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"âœ… ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_filename}\")\n",
    "\n",
    "# Colabì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(output_filename)\n",
    "    print(f\"âœ… íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {output_filename}\")\n",
    "except ImportError:\n",
    "    print(\"ë¡œì»¬ í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. íŒŒì¼ì´ í˜„ì¬ ë””ë ‰í„°ë¦¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "print(\"ğŸ”¥ ëª¨ë“  ê³¼ì • ì™„ë£Œ!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
