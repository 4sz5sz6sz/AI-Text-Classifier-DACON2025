{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI vs 인간 생성 텍스트 분류기 (LLM 기반 - 한국어 Colab 최적화)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Google Colab 실행 가이드 및 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa14ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 설치 (Colab 환경에 최적화)\n",
    "!pip install -q transformers accelerate bitsandbytes peft datasets sentence-transformers\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 # T4 GPU 지원 PyTorch\n",
    "!pip install -q gdown # Google Drive 파일 다운로드용\n",
    "!pip install -q psutil # 메모리 사용량 모니터링용\n",
    "\n",
    "print(\"✅ 라이브러리 설치 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import 및 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import gc\n",
    "import psutil\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from accelerate import Accelerator\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "\n",
    "print(\"✅ 필요한 모듈 임포트 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚙️ GPU 및 메모리 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "    return memory_mb\n",
    "\n",
    "print(f\"🔍 시작 시 메모리 사용량: {get_memory_usage():.1f}MB\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"✅ GPU 사용 가능: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   - 총 메모리: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"⚠️ GPU 사용 불가 - CPU 모드로 전환합니다.\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"🧹 초기 메모리 정리 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📥 데이터 로드 (Google Drive 자동 다운로드)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "\n",
    "# 파일 ID 설정 (Google Drive 공개 링크에서 추출)\n",
    "# 실제 대회 데이터셋의 Google Drive ID로 교체해야 합니다.\n",
    "TRAIN_FILE_ID = \"1teA9GmYlIsutaDLWvCCsLeh7833t-TC_\" # 예시 ID, 실제 ID로 변경 필요\n",
    "TEST_FILE_ID = \"1bGC_YWtNUOroHARfmzCjrL8oPcvb7Tpw\"  # 예시 ID, 실제 ID로 변경 필요\n",
    "SAMPLE_FILE_ID = \"1ebrHVj-CtM-7aEz4OqP-bCHlazle-PKM\" # 예시 ID, 실제 ID로 변경 필요\n",
    "\n",
    "def download_from_drive(file_id, filename):\n",
    "    if file_id and not os.path.exists(filename):\n",
    "        url = f'https://drive.google.com/uc?id={file_id}'\n",
    "        print(f\"📥 {filename} 다운로드 중...\")\n",
    "        gdown.download(url, filename, quiet=False)\n",
    "        print(f\"✅ {filename} 다운로드 완료!\")\n",
    "    elif os.path.exists(filename):\n",
    "        print(f\"✅ {filename} 이미 존재함\")\n",
    "    else:\n",
    "        print(f\"⚠️ {filename} 파일 ID가 없거나 파일이 존재하지 않습니다. 수동 다운로드 필요.\")\n",
    "\n",
    "# 자동 다운로드 실행\n",
    "print(\"📁 데이터 파일 로딩 시작...\")\n",
    "download_from_drive(TRAIN_FILE_ID, 'train.csv')\n",
    "download_from_drive(TEST_FILE_ID, 'test.csv')\n",
    "download_from_drive(SAMPLE_FILE_ID, 'sample_submission.csv')\n",
    "\n",
    "# CSV 파일 읽기\n",
    "try:\n",
    "    train_df = pd.read_csv('train.csv', encoding='utf-8-sig')\n",
    "    test_df = pd.read_csv('test.csv', encoding='utf-8-sig')\n",
    "    sample_submission_df = pd.read_csv('sample_submission.csv', encoding='utf-8-sig')\n",
    "    print(\"✅ 데이터셋 로드 완료!\")\n",
    "    print(f\"   - Train shape: {train_df.shape}\")\n",
    "    print(f\"   - Test shape: {test_df.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 데이터 로드 중 오류 발생: {e}\")\n",
    "    print(\"   - 파일이 올바르게 다운로드되었는지 확인해주세요.\")\n",
    "    print(\"   - 인코딩 문제일 수 있으니, 다른 인코딩을 시도해보세요 (예: cp949, euc-kr).\")\n",
    "    raise\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"🧹 데이터 로드 후 메모리 정리 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 데이터 전처리 및 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'paragraph_text' 컬럼을 'full_text'로 통일 (필요한 경우)\n",
    "if 'paragraph_text' in test_df.columns and 'full_text' not in test_df.columns:\n",
    "    test_df = test_df.rename(columns={'paragraph_text': 'full_text'})\n",
    "    print(\"✅ Test 데이터의 'paragraph_text' 컬럼을 'full_text'로 변경했습니다.\")\n",
    "\n",
    "# 훈련 데이터와 검증 데이터 분할\n",
    "X = train_df[['title', 'full_text']]\n",
    "y = train_df['generated']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.05, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"✅ 훈련 세트: {X_train.shape}, 검증 세트: {X_val.shape}\")\n",
    "print(f\"   - 훈련 세트 AI 비율: {y_train.mean():.3f}\")\n",
    "print(f\"   - 검증 세트 AI 비율: {y_val.mean():.3f}\")\n",
    "\n",
    "# NumPy 2.0 호환 - PyTorch Dataset 클래스 직접 구현\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "\n",
    "class TextDataset(TorchDataset):\n",
    "    def __init__(self, texts, labels=None):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {'text': str(self.texts[idx])}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = int(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "# 텍스트 데이터 준비 (NumPy 배열 사용 안함)\n",
    "train_texts = [str(title) + ' ' + str(text) for title, text in zip(X_train['title'], X_train['full_text'])]\n",
    "val_texts = [str(title) + ' ' + str(text) for title, text in zip(X_val['title'], X_val['full_text'])]\n",
    "test_texts = [str(title) + ' ' + str(text) for title, text in zip(test_df['title'], test_df['full_text'])]\n",
    "\n",
    "# 라벨 준비 (NumPy 배열 사용 안함)\n",
    "train_labels = y_train.tolist()\n",
    "val_labels = y_val.tolist()\n",
    "\n",
    "# PyTorch Dataset 생성\n",
    "torch_train_dataset = TextDataset(train_texts, train_labels)\n",
    "torch_val_dataset = TextDataset(val_texts, val_labels)\n",
    "torch_test_dataset = TextDataset(test_texts)\n",
    "\n",
    "print(\"✅ PyTorch Dataset 생성 완료!\")\n",
    "\n",
    "# 토크나이저 로드 (한국어 모델)\n",
    "MODEL_NAME = \"klue/roberta-base\" # 한국어 RoBERTa 모델\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Hugging Face Dataset으로 변환 (NumPy 2.0 호환)\n",
    "def convert_to_hf_dataset(torch_dataset):\n",
    "    # 모든 데이터를 한 번에 변환\n",
    "    data_dict = {'text': []}\n",
    "    if hasattr(torch_dataset, 'labels') and torch_dataset.labels is not None:\n",
    "        data_dict['labels'] = []\n",
    "    \n",
    "    for i in range(len(torch_dataset)):\n",
    "        item = torch_dataset[i]\n",
    "        data_dict['text'].append(item['text'])\n",
    "        if 'labels' in item:\n",
    "            data_dict['labels'].append(item['labels'])\n",
    "    \n",
    "    return Dataset.from_dict(data_dict)\n",
    "\n",
    "train_dataset = convert_to_hf_dataset(torch_train_dataset)\n",
    "val_dataset = convert_to_hf_dataset(torch_val_dataset)\n",
    "test_dataset = convert_to_hf_dataset(torch_test_dataset)\n",
    "\n",
    "# 토큰화 함수\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=512, padding=False)\n",
    "\n",
    "print(\"⏳ 데이터 토큰화 중...\")\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# PyTorch 형식으로 설정\n",
    "tokenized_train_dataset.set_format(\"torch\")\n",
    "tokenized_val_dataset.set_format(\"torch\")\n",
    "tokenized_test_dataset.set_format(\"torch\")\n",
    "\n",
    "print(\"✅ 데이터 토큰화 및 형식 설정 완료!\")\n",
    "print(f\"   - 훈련 데이터셋: {len(tokenized_train_dataset)} 샘플\")\n",
    "print(f\"   - 검증 데이터셋: {len(tokenized_val_dataset)} 샘플\")\n",
    "print(f\"   - 테스트 데이터셋: {len(tokenized_test_dataset)} 샘플\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"🧹 토큰화 후 메모리 정리 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧠 모델 로드 및 PEFT (LoRA) 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4비트 양자화 설정 (메모리 절약)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 # T4 GPU는 bfloat16 지원\n",
    ")\n",
    "\n",
    "# 모델 로드 (Sequence Classification용)\n",
    "print(f\"⏳ {MODEL_NAME} 모델 로드 중...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=1, # 이진 분류 (0 또는 1)\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16, # 모델 dtype 설정\n",
    ")\n",
    "\n",
    "print(\"✅ 모델 로드 완료!\")\n",
    "\n",
    "# LoRA 설정 (PEFT)\n",
    "lora_config = LoraConfig(\n",
    "    r=16, # LoRA 랭크 (llm-detect-ai의 v26 설정 참고)\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    target_modules=[\"query\", \"key\", \"value\"] # RoBERTa 모델의 어텐션 레이어\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"✅ LoRA 설정 완료!\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"🧹 모델 로드 및 LoRA 설정 후 메모리 정리 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련 파라미터 및 DataLoader 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# 데이터 콜레이터 (동적 패딩)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# DataLoader 설정\n",
    "BATCH_SIZE = 8 # T4 GPU에 적합한 배치 사이즈, 필요시 조절\n",
    "GRADIENT_ACCUMULATION_STEPS = 4 # 경사 누적 단계 (실제 배치 사이즈 = BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS)\n",
    "NUM_EPOCHS = 3 # 훈련 에포크 수\n",
    "LEARNING_RATE = 2e-5 # 학습률\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_val_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    tokenized_test_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "print(\"✅ DataLoader 설정 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련 루프 (Accelerate 활용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "accelerator = Accelerator(gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "model, optimizer, train_dataloader, eval_dataloader, test_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader, test_dataloader\n",
    ")\n",
    "\n",
    "num_training_steps = NUM_EPOCHS * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"cosine\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=int(num_training_steps * 0.1),\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "print(\"🚀 훈련 시작!\")\n",
    "progress_bar = tqdm(range(num_training_steps), disable=not accelerator.is_local_main_process)\n",
    "\n",
    "model.train()\n",
    "best_auc = -1.0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        with accelerator.accumulate(model):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if accelerator.sync_gradients:\n",
    "            progress_bar.update(1)\n",
    "            if (step + 1) % 100 == 0: # 100 스텝마다 로깅\n",
    "                accelerator.print(f\"Epoch {epoch+1}, Step {step+1}/{len(train_dataloader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # 에포크 종료 후 검증\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for batch in eval_dataloader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        predictions = torch.sigmoid(outputs.logits).squeeze().cpu().numpy()\n",
    "        labels = batch[\"labels\"] .cpu().numpy()\n",
    "        all_preds.extend(predictions)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "    auc_score = roc_auc_score(all_labels, all_preds)\n",
    "    accelerator.print(f\"Epoch {epoch+1} 검증 AUC: {auc_score:.4f}\")\n",
    "\n",
    "    if auc_score > best_auc:\n",
    "        best_auc = auc_score\n",
    "        accelerator.save_state(\"./best_model_checkpoint\")\n",
    "        accelerator.print(f\"🎉 새로운 최고 AUC 달성! 모델 저장됨: {best_auc:.4f}\")\n",
    "\n",
    "    model.train()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "accelerator.print(\"✅ 훈련 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔮 예측 (Inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적 모델 로드 (훈련이 완료된 후)\n",
    "print(\"⏳ 최적 모델 로드 중...\")\n",
    "accelerator.load_state(\"./best_model_checkpoint\")\n",
    "model.eval()\n",
    "\n",
    "all_test_preds = []\n",
    "print(\"🔮 테스트 데이터 예측 중...\")\n",
    "for batch in tqdm(test_dataloader, disable=not accelerator.is_local_main_process):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    predictions = torch.sigmoid(outputs.logits).squeeze().cpu().numpy()\n",
    "    all_test_preds.extend(predictions)\n",
    "\n",
    "probs = np.array(all_test_preds)\n",
    "\n",
    "print(\"✅ 예측 완료!\")\n",
    "print(f\"   - 예측값 개수: {len(probs)}\")\n",
    "print(f\"   - 예측값 범위: [{probs.min():.3f}, {probs.max():.3f}]\")\n",
    "print(f\"   - 예측값 평균: {probs.mean():.3f}\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"🧹 예측 후 메모리 정리 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📤 제출 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_submission 파일 읽기 (이미 로드되어 있을 수 있음)\n",
    "if 'sample_submission_df' not in locals():\n",
    "    sample_submission_df = pd.read_csv('sample_submission.csv', encoding='utf-8-sig')\n",
    "\n",
    "sample_submission_df['generated'] = probs\n",
    "\n",
    "output_filename = 'submission_llm_korean_colab.csv'\n",
    "sample_submission_df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"✅ 제출 파일 저장 완료: {output_filename}\")\n",
    "\n",
    "# Colab에서 파일 다운로드\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(output_filename)\n",
    "    print(f\"✅ 파일 다운로드 시작: {output_filename}\")\n",
    "except ImportError:\n",
    "    print(\"로컬 환경에서 실행 중입니다. 파일이 현재 디렉터리에 저장되었습니다.\")\n",
    "\n",
    "print(\"🔥 모든 과정 완료!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
