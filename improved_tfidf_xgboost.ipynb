{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e74b97e",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/4sz5sz6sz/AI-Text-Classifier-DACON2025/blob/main/improved_tfidf_xgboost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e16bb3",
   "metadata": {
    "id": "f3e16bb3"
   },
   "source": [
    "# TF-IDF + XGBoost ê°œì„  ì‹¤í—˜\n",
    "\n",
    "**ê°œì„  ì‚¬í•­:**\n",
    "- í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì¶”ê°€\n",
    "- êµì°¨ ê²€ì¦ ì ìš©\n",
    "- í”¼ì²˜ ê°œìˆ˜ ì¡°ì •\n",
    "- ì„±ëŠ¥ ë¹„êµ ë¶„ì„"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b01a364",
   "metadata": {},
   "source": [
    "## ğŸš€ Google Colab ì‹¤í–‰ ê°€ì´ë“œ\n",
    "\n",
    "## íŒŒì¼ ì—…ë¡œë“œ ë°©ë²•\n",
    "\n",
    "**ëŒ€ìš©ëŸ‰ íŒŒì¼(500MB+)**: Google Drive ê³µê°œ ë§í¬ ì‚¬ìš© (ì•„ë˜ ì½”ë“œ ì°¸ê³ )\n",
    "**ì†Œìš©ëŸ‰ íŒŒì¼**: ì§ì ‘ ì—…ë¡œë“œ\n",
    "\n",
    "**ì£¼ì˜ì‚¬í•­:**\n",
    "- íŒŒì¼ ì—…ë¡œë“œëŠ” í•œ ë²ˆë§Œ í•˜ë©´ ë©ë‹ˆë‹¤\n",
    "- ëŸ°íƒ€ì„ì´ ì¬ì‹œì‘ë˜ë©´ ë‹¤ì‹œ ì—…ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤\n",
    "- ì¸ì½”ë”© ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ ìë™ìœ¼ë¡œ ì—¬ëŸ¬ ë°©ì‹ì„ ì‹œë„í•©ë‹ˆë‹¤\n",
    "\n",
    "**ì‹¤í–‰ ìˆœì„œ:**\n",
    "1. ëª¨ë“  ì…€ì„ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰\n",
    "2. íŒŒì¼ ì—…ë¡œë“œê°€ ìš”ì²­ë˜ë©´ ë°ì´í„° íŒŒì¼ë“¤ì„ ì—…ë¡œë“œ\n",
    "3. ìµœì¢… ê²°ê³¼ëŠ” ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œë©ë‹ˆë‹¤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d8caaf",
   "metadata": {
    "id": "69d8caaf"
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcbb6404",
   "metadata": {
    "id": "dcbb6404"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b143f580",
   "metadata": {
    "id": "b143f580"
   },
   "source": [
    "# Data Load & Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4zGT3sBgtRHM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4zGT3sBgtRHM",
    "outputId": "a579bdde-4e89-4e86-9543-32e8c8caa07a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== train.csv ì½ê¸° ===\n",
      "'utf-8-sig' ì¸ì½”ë”©ìœ¼ë¡œ ì‹œë„ ì¤‘...\n",
      "'utf-8-sig' ì¸ì½”ë”©ì—ì„œ ë‹¤ë¥¸ ì˜¤ë¥˜: Error tokenizing data. C error: out of memory\n",
      "'utf-8' ì¸ì½”ë”©ìœ¼ë¡œ ì‹œë„ ì¤‘...\n",
      "'utf-8-sig' ì¸ì½”ë”©ì—ì„œ ë‹¤ë¥¸ ì˜¤ë¥˜: Error tokenizing data. C error: out of memory\n",
      "'utf-8' ì¸ì½”ë”©ìœ¼ë¡œ ì‹œë„ ì¤‘...\n",
      "ì„±ê³µ! 'utf-8' ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ì„ ì½ì—ˆìŠµë‹ˆë‹¤.\n",
      "Train ë°ì´í„° shape: (97172, 3)\n",
      "\n",
      "=== test.csv ì½ê¸° ===\n",
      "'utf-8-sig' ì¸ì½”ë”©ìœ¼ë¡œ ì‹œë„ ì¤‘...\n",
      "ì„±ê³µ! 'utf-8-sig' ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ì„ ì½ì—ˆìŠµë‹ˆë‹¤.\n",
      "Test ë°ì´í„° shape: (1962, 4)\n",
      "ì„±ê³µ! 'utf-8' ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ì„ ì½ì—ˆìŠµë‹ˆë‹¤.\n",
      "Train ë°ì´í„° shape: (97172, 3)\n",
      "\n",
      "=== test.csv ì½ê¸° ===\n",
      "'utf-8-sig' ì¸ì½”ë”©ìœ¼ë¡œ ì‹œë„ ì¤‘...\n",
      "ì„±ê³µ! 'utf-8-sig' ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ì„ ì½ì—ˆìŠµë‹ˆë‹¤.\n",
      "Test ë°ì´í„° shape: (1962, 4)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "# ì½”ë©ì—ì„œ íŒŒì¼ ì—…ë¡œë“œ (ì²˜ìŒ ì‹¤í–‰ ì‹œì—ë§Œ)\n",
    "def upload_files_if_needed():\n",
    "    required_files = ['train.csv', 'test.csv', 'sample_submission.csv']\n",
    "    missing_files = [f for f in required_files if not os.path.exists(f)]\n",
    "    \n",
    "    if missing_files:\n",
    "        print(f\"ë‹¤ìŒ íŒŒì¼ë“¤ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”: {missing_files}\")\n",
    "        uploaded = files.upload()\n",
    "        print(\"íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ!\")\n",
    "        return uploaded\n",
    "    else:\n",
    "        print(\"ëª¨ë“  í•„ìš”í•œ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.\")\n",
    "        return None\n",
    "\n",
    "# ê°•í™”ëœ CSV ì½ê¸° í•¨ìˆ˜\n",
    "def read_csv_robust(file_path):\n",
    "    \"\"\"ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ CSV íŒŒì¼ì„ ì½ì–´ë³´ëŠ” í•¨ìˆ˜\"\"\"\n",
    "    \n",
    "    # ë°©ë²• 1: ë‹¤ì–‘í•œ ì¸ì½”ë”© ì‹œë„\n",
    "    encodings = ['utf-8', 'utf-8-sig', 'cp949', 'euc-kr', 'latin-1', 'iso-8859-1']\n",
    "    \n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            print(f\"'{encoding}' ì¸ì½”ë”©ìœ¼ë¡œ ì‹œë„ ì¤‘...\")\n",
    "            df = pd.read_csv(file_path, encoding=encoding)\n",
    "            print(f\"âœ… ì„±ê³µ! '{encoding}' ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ì„ ì½ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "            return df\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"âŒ '{encoding}' ì¸ì½”ë”© ì‹¤íŒ¨\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ '{encoding}' ì¸ì½”ë”©ì—ì„œ ì˜¤ë¥˜: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # ë°©ë²• 2: ë°”ì´ë„ˆë¦¬ ëª¨ë“œë¡œ ì½ì–´ì„œ ì¸ì½”ë”© ê°ì§€\n",
    "    try:\n",
    "        print(\"ì¸ì½”ë”© ìë™ ê°ì§€ë¥¼ ì‹œë„í•©ë‹ˆë‹¤...\")\n",
    "        import chardet\n",
    "        \n",
    "        with open(file_path, 'rb') as f:\n",
    "            raw_data = f.read()\n",
    "            result = chardet.detect(raw_data)\n",
    "            encoding = result['encoding']\n",
    "            confidence = result['confidence']\n",
    "            \n",
    "        print(f\"ê°ì§€ëœ ì¸ì½”ë”©: {encoding} (ì‹ ë¢°ë„: {confidence:.2f})\")\n",
    "        \n",
    "        if confidence > 0.7:  # ì‹ ë¢°ë„ê°€ 70% ì´ìƒì¸ ê²½ìš°ë§Œ\n",
    "            df = pd.read_csv(file_path, encoding=encoding)\n",
    "            print(f\"âœ… ìë™ ê°ì§€ëœ ì¸ì½”ë”©ìœ¼ë¡œ ì„±ê³µ!\")\n",
    "            return df\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"chardet ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. ì„¤ì¹˜ë¥¼ ì‹œë„í•©ë‹ˆë‹¤...\")\n",
    "        !pip install chardet\n",
    "        import chardet\n",
    "        # ìœ„ ì½”ë“œ ì¬ì‹¤í–‰\n",
    "        with open(file_path, 'rb') as f:\n",
    "            raw_data = f.read()\n",
    "            result = chardet.detect(raw_data)\n",
    "            encoding = result['encoding']\n",
    "            \n",
    "        df = pd.read_csv(file_path, encoding=encoding)\n",
    "        print(f\"âœ… chardet ì„¤ì¹˜ í›„ ì„±ê³µ!\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ìë™ ê°ì§€ ì‹¤íŒ¨: {e}\")\n",
    "    \n",
    "    # ë°©ë²• 3: ì—ëŸ¬ ì²˜ë¦¬ ì˜µì…˜ ì‚¬ìš©\n",
    "    try:\n",
    "        print(\"ì—ëŸ¬ ë¬´ì‹œ ëª¨ë“œë¡œ ì‹œë„í•©ë‹ˆë‹¤...\")\n",
    "        df = pd.read_csv(file_path, encoding='utf-8', errors='ignore')\n",
    "        print(\"âœ… ì—ëŸ¬ ë¬´ì‹œ ëª¨ë“œë¡œ ì„±ê³µ!\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì—ëŸ¬ ë¬´ì‹œ ëª¨ë“œ ì‹¤íŒ¨: {e}\")\n",
    "    \n",
    "    raise Exception(\"âŒ ëª¨ë“  ë°©ë²•ìœ¼ë¡œ íŒŒì¼ì„ ì½ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# íŒŒì¼ ì—…ë¡œë“œ (í•„ìš”í•œ ê²½ìš°)\n",
    "uploaded = upload_files_if_needed()\n",
    "\n",
    "# íŒŒì¼ ì½ê¸°\n",
    "try:\n",
    "    print(\"\\n=== train.csv ì½ê¸° ===\")\n",
    "    train = read_csv_robust('train.csv')\n",
    "    print(f\"Train ë°ì´í„° shape: {train.shape}\")\n",
    "    print(f\"Train ì»¬ëŸ¼: {train.columns.tolist()}\")\n",
    "\n",
    "    print(\"\\n=== test.csv ì½ê¸° ===\")\n",
    "    test = read_csv_robust('test.csv')\n",
    "    print(f\"Test ë°ì´í„° shape: {test.shape}\")\n",
    "    print(f\"Test ì»¬ëŸ¼: {test.columns.tolist()}\")\n",
    "\n",
    "    print(\"\\n=== sample_submission.csv ì½ê¸° ===\")\n",
    "    sample_submission = read_csv_robust('sample_submission.csv')\n",
    "    print(f\"Sample submission shape: {sample_submission.shape}\")\n",
    "    print(f\"Sample submission ì»¬ëŸ¼: {sample_submission.columns.tolist()}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}\")\n",
    "    print(\"ë‹¤ìŒì„ í™•ì¸í•´ì£¼ì„¸ìš”:\")\n",
    "    print(\"1. íŒŒì¼ì´ ì˜¬ë°”ë¥´ê²Œ ì—…ë¡œë“œë˜ì—ˆëŠ”ì§€\")\n",
    "    print(\"2. íŒŒì¼ëª…ì´ ì •í™•í•œì§€ (train.csv, test.csv, sample_submission.csv)\")\n",
    "    print(\"3. íŒŒì¼ì´ ì†ìƒë˜ì§€ ì•Šì•˜ëŠ”ì§€\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee25cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëŒ€ìš©ëŸ‰ íŒŒì¼ ë‹¤ìš´ë¡œë“œ (Google Drive ê³µê°œ ë§í¬)\n",
    "import gdown\n",
    "\n",
    "# íŒŒì¼ ID ì„¤ì • (Google Drive ë§í¬ì—ì„œ ì¶”ì¶œ)\n",
    "TRAIN_FILE_ID = \"1teA9GmYlIsutaDLWvCCsLeh7833t-TC_\"  # ì œê³µë°›ì€ train.csv ID\n",
    "TEST_FILE_ID = \"\"     # test.csv ID (í•„ìš”ì‹œ ì…ë ¥)\n",
    "SAMPLE_FILE_ID = \"\"   # sample_submission.csv ID (í•„ìš”ì‹œ ì…ë ¥)\n",
    "\n",
    "def download_from_drive(file_id, filename):\n",
    "    if file_id:\n",
    "        url = f'https://drive.google.com/uc?id={file_id}'\n",
    "        gdown.download(url, filename, quiet=False)\n",
    "        print(f\"âœ… {filename} ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!\")\n",
    "\n",
    "# íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤í–‰\n",
    "if TRAIN_FILE_ID and not os.path.exists('train.csv'):\n",
    "    print(\"ğŸ“¥ train.csv ë‹¤ìš´ë¡œë“œ ì¤‘...\")\n",
    "    download_from_drive(TRAIN_FILE_ID, 'train.csv')\n",
    "    \n",
    "if TEST_FILE_ID and not os.path.exists('test.csv'):\n",
    "    download_from_drive(TEST_FILE_ID, 'test.csv')\n",
    "    \n",
    "if SAMPLE_FILE_ID and not os.path.exists('sample_submission.csv'):\n",
    "    download_from_drive(SAMPLE_FILE_ID, 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b411202",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "6b411202",
    "outputId": "cbee5210-f6fd-4775-e951-e87ade0ecadb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (97172, 3)\n",
      "Test shape: (1962, 4)\n",
      "Generated ë¹„ìœ¨: 0.082\n"
     ]
    }
   ],
   "source": [
    "#train = pd.read_csv('./train.csv', encoding='utf-8-sig')\n",
    "#test = pd.read_csv('./test.csv', encoding='utf-8-sig')\n",
    "\n",
    "# ë°ì´í„° í™•ì¸\n",
    "try:\n",
    "    print(\"=== ë°ì´í„° ê¸°ë³¸ ì •ë³´ ===\")\n",
    "    print(f\"Train shape: {train.shape}\")\n",
    "    print(f\"Test shape: {test.shape}\")\n",
    "    \n",
    "    # ì»¬ëŸ¼ í™•ì¸\n",
    "    print(f\"\\nTrain ì»¬ëŸ¼: {train.columns.tolist()}\")\n",
    "    print(f\"Test ì»¬ëŸ¼: {test.columns.tolist()}\")\n",
    "    \n",
    "    # target ë³€ìˆ˜ í™•ì¸\n",
    "    if 'generated' in train.columns:\n",
    "        print(f\"\\nGenerated ë¹„ìœ¨: {train['generated'].mean():.3f}\")\n",
    "        print(f\"Generated ê°’ ë¶„í¬:\")\n",
    "        print(train['generated'].value_counts())\n",
    "    else:\n",
    "        print(\"\\nâš ï¸ 'generated' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        print(\"ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼:\", train.columns.tolist())\n",
    "    \n",
    "    # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°\n",
    "    print(\"\\n=== Train ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° ===\")\n",
    "    print(train.head())\n",
    "    \n",
    "    print(\"\\n=== Test ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° ===\")\n",
    "    print(test.head())\n",
    "    \n",
    "    # ê²°ì¸¡ê°’ í™•ì¸\n",
    "    print(\"\\n=== ê²°ì¸¡ê°’ í™•ì¸ ===\")\n",
    "    print(\"Train ê²°ì¸¡ê°’:\")\n",
    "    print(train.isnull().sum())\n",
    "    print(\"\\nTest ê²°ì¸¡ê°’:\")\n",
    "    print(test.isnull().sum())\n",
    "    \n",
    "except NameError:\n",
    "    print(\"âŒ ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìœ„ì˜ ì…€ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ë°ì´í„° í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9c6724",
   "metadata": {
    "id": "cf9c6724"
   },
   "outputs": [],
   "source": [
    "# ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¶„í• \n",
    "try:\n",
    "    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì»¬ëŸ¼ëª… í†µì¼ (í•„ìš”í•œ ê²½ìš°)\n",
    "    if 'paragraph_text' in test.columns and 'full_text' not in test.columns:\n",
    "        test = test.rename(columns={'paragraph_text': 'full_text'})\n",
    "        print(\"âœ… Test ë°ì´í„°ì˜ 'paragraph_text' ì»¬ëŸ¼ì„ 'full_text'ë¡œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    # í•„ìš”í•œ ì»¬ëŸ¼ í™•ì¸\n",
    "    required_train_cols = ['title', 'full_text', 'generated']\n",
    "    required_test_cols = ['title', 'full_text']\n",
    "    \n",
    "    missing_train_cols = [col for col in required_train_cols if col not in train.columns]\n",
    "    missing_test_cols = [col for col in required_test_cols if col not in test.columns]\n",
    "    \n",
    "    if missing_train_cols:\n",
    "        print(f\"âŒ Train ë°ì´í„°ì—ì„œ ëˆ„ë½ëœ ì»¬ëŸ¼: {missing_train_cols}\")\n",
    "        print(f\"ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {train.columns.tolist()}\")\n",
    "        raise ValueError(\"í•„ìš”í•œ ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    if missing_test_cols:\n",
    "        print(f\"âŒ Test ë°ì´í„°ì—ì„œ ëˆ„ë½ëœ ì»¬ëŸ¼: {missing_test_cols}\")\n",
    "        print(f\"ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {test.columns.tolist()}\")\n",
    "        raise ValueError(\"í•„ìš”í•œ ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    # ë°ì´í„° ë¶„í• \n",
    "    X = train[['title', 'full_text']]\n",
    "    y = train['generated']\n",
    "    \n",
    "    print(f\"âœ… í”¼ì²˜ ë°ì´í„° shape: {X.shape}\")\n",
    "    print(f\"âœ… íƒ€ê²Ÿ ë°ì´í„° shape: {y.shape}\")\n",
    "    \n",
    "    # Train-Validation ë¶„í• \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, stratify=y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… í›ˆë ¨ ì„¸íŠ¸: {X_train.shape}\")\n",
    "    print(f\"âœ… ê²€ì¦ ì„¸íŠ¸: {X_val.shape}\")\n",
    "    print(f\"âœ… í›ˆë ¨ ì„¸íŠ¸ íƒ€ê²Ÿ ë¹„ìœ¨: {y_train.mean():.3f}\")\n",
    "    print(f\"âœ… ê²€ì¦ ì„¸íŠ¸ íƒ€ê²Ÿ ë¹„ìœ¨: {y_val.mean():.3f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ë°ì´í„° ì „ì²˜ë¦¬ ì˜¤ë¥˜: {e}\")\n",
    "    print(\"ë°ì´í„° êµ¬ì¡°ë¥¼ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc44e7d",
   "metadata": {
    "id": "fbc44e7d"
   },
   "source": [
    "# ê°œì„ ëœ TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3c9cafe",
   "metadata": {
    "id": "f3c9cafe"
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      6\u001b[39m vectorizer = FeatureUnion([\n\u001b[32m      7\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m, Pipeline([(\u001b[33m'\u001b[39m\u001b[33mselector\u001b[39m\u001b[33m'\u001b[39m, get_title),\n\u001b[32m      8\u001b[39m                         (\u001b[33m'\u001b[39m\u001b[33mtfidf\u001b[39m\u001b[33m'\u001b[39m, TfidfVectorizer(ngram_range=(\u001b[32m1\u001b[39m,\u001b[32m3\u001b[39m), max_features=\u001b[32m5000\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m                                                     min_df=\u001b[32m2\u001b[39m, max_df=\u001b[32m0.95\u001b[39m))])),\n\u001b[32m     13\u001b[39m ])\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# í”¼ì²˜ ë³€í™˜\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m X_train_vec = \u001b[43mvectorizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m X_val_vec = vectorizer.transform(X_val)\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mí”¼ì²˜ ì°¨ì›: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train_vec.shape[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    322\u001b[39m         return_tuple = (\n\u001b[32m    323\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    324\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    325\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\pipeline.py:1974\u001b[39m, in \u001b[36mFeatureUnion.fit_transform\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1971\u001b[39m             routed_params[name] = Bunch(transform={})\n\u001b[32m   1972\u001b[39m             routed_params[name].fit = params\n\u001b[32m-> \u001b[39m\u001b[32m1974\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parallel_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_fit_transform_one\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1975\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m results:\n\u001b[32m   1976\u001b[39m     \u001b[38;5;66;03m# All transformers are None\u001b[39;00m\n\u001b[32m   1977\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.zeros((X.shape[\u001b[32m0\u001b[39m], \u001b[32m0\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\pipeline.py:1996\u001b[39m, in \u001b[36mFeatureUnion._parallel_func\u001b[39m\u001b[34m(self, X, y, func, routed_params)\u001b[39m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_transformer_weights()\n\u001b[32m   1994\u001b[39m transformers = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m._iter())\n\u001b[32m-> \u001b[39m\u001b[32m1996\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1997\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1998\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2000\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2001\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFeatureUnion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtransformers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2005\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2006\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtransformers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2007\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1916\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1917\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1918\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1920\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1921\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1922\u001b[39m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1923\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1924\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1925\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1845\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1846\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1847\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1848\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1849\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:139\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m     config = {}\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config):\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\pipeline.py:1551\u001b[39m, in \u001b[36m_fit_transform_one\u001b[39m\u001b[34m(transformer, X, y, weight, message_clsname, message, params)\u001b[39m\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[32m   1550\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[33m\"\u001b[39m\u001b[33mfit_transform\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1551\u001b[39m         res = \u001b[43mtransformer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit_transform\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1552\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1553\u001b[39m         res = transformer.fit(X, y, **params.get(\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m, {})).transform(\n\u001b[32m   1554\u001b[39m             X, **params.get(\u001b[33m\"\u001b[39m\u001b[33mtransform\u001b[39m\u001b[33m\"\u001b[39m, {})\n\u001b[32m   1555\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\pipeline.py:730\u001b[39m, in \u001b[36mPipeline.fit_transform\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    724\u001b[39m last_step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    725\u001b[39m     step_idx=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) - \u001b[32m1\u001b[39m,\n\u001b[32m    726\u001b[39m     step_params=routed_params[\u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]],\n\u001b[32m    727\u001b[39m     all_params=params,\n\u001b[32m    728\u001b[39m )\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(last_step, \u001b[33m\"\u001b[39m\u001b[33mfit_transform\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m730\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlast_step\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m        \u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit_transform\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m last_step.fit(Xt, y, **last_step_params[\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m]).transform(\n\u001b[32m    735\u001b[39m         Xt, **last_step_params[\u001b[33m\"\u001b[39m\u001b[33mtransform\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    736\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2104\u001b[39m, in \u001b[36mTfidfVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   2097\u001b[39m \u001b[38;5;28mself\u001b[39m._check_params()\n\u001b[32m   2098\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf = TfidfTransformer(\n\u001b[32m   2099\u001b[39m     norm=\u001b[38;5;28mself\u001b[39m.norm,\n\u001b[32m   2100\u001b[39m     use_idf=\u001b[38;5;28mself\u001b[39m.use_idf,\n\u001b[32m   2101\u001b[39m     smooth_idf=\u001b[38;5;28mself\u001b[39m.smooth_idf,\n\u001b[32m   2102\u001b[39m     sublinear_tf=\u001b[38;5;28mself\u001b[39m.sublinear_tf,\n\u001b[32m   2103\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2104\u001b[39m X = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2105\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf.fit(X)\n\u001b[32m   2106\u001b[39m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[32m   2107\u001b[39m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1376\u001b[39m, in \u001b[36mCountVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   1368\u001b[39m             warnings.warn(\n\u001b[32m   1369\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mUpper case characters found in\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1370\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m vocabulary while \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlowercase\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1371\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m is True. These entries will not\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1372\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m be matched with any documents\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1373\u001b[39m             )\n\u001b[32m   1374\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1376\u001b[39m vocabulary, X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.binary:\n\u001b[32m   1379\u001b[39m     X.data.fill(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1265\u001b[39m, in \u001b[36mCountVectorizer._count_vocab\u001b[39m\u001b[34m(self, raw_documents, fixed_vocab)\u001b[39m\n\u001b[32m   1263\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m analyze(doc):\n\u001b[32m   1264\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1265\u001b[39m         feature_idx = \u001b[43mvocabulary\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   1266\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m feature_idx \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m feature_counter:\n\u001b[32m   1267\u001b[39m             feature_counter[feature_idx] = \u001b[32m1\u001b[39m\n",
      "\u001b[31mMemoryError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# í”¼ì²˜ ê°œìˆ˜ ëŠ˜ë¦¬ê³  ë‹¤ì–‘í•œ n-gram ì‹œë„\n",
    "get_title = FunctionTransformer(lambda x: x['title'], validate=False)\n",
    "get_text = FunctionTransformer(lambda x: x['full_text'], validate=False)\n",
    "\n",
    "# ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë²¡í„°í™” (í”¼ì²˜ ê°œìˆ˜ ì¡°ì •)\n",
    "vectorizer = FeatureUnion([\n",
    "    ('title', Pipeline([('selector', get_title),\n",
    "                        ('tfidf', TfidfVectorizer(ngram_range=(1,2), max_features=2000,\n",
    "                                                min_df=3, max_df=0.9))])),\n",
    "    ('full_text', Pipeline([('selector', get_text),\n",
    "                            ('tfidf', TfidfVectorizer(ngram_range=(1,2), max_features=8000,\n",
    "                                                    min_df=3, max_df=0.9))])),\n",
    "])\n",
    "\n",
    "# í”¼ì²˜ ë³€í™˜\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_val_vec = vectorizer.transform(X_val)\n",
    "\n",
    "print(f\"í”¼ì²˜ ì°¨ì›: {X_train_vec.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf606fd",
   "metadata": {
    "id": "7bf606fd"
   },
   "source": [
    "# ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ (ë¹„êµìš©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488bae9f",
   "metadata": {
    "id": "488bae9f"
   },
   "outputs": [],
   "source": [
    "# ê¸°ë³¸ XGBoost (ë² ì´ìŠ¤ë¼ì¸)\n",
    "xgb_baseline = XGBClassifier(random_state=42)\n",
    "xgb_baseline.fit(X_train_vec, y_train)\n",
    "\n",
    "val_probs_baseline = xgb_baseline.predict_proba(X_val_vec)[:, 1]\n",
    "auc_baseline = roc_auc_score(y_val, val_probs_baseline)\n",
    "print(f\"ë² ì´ìŠ¤ë¼ì¸ Validation AUC: {auc_baseline:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab40749",
   "metadata": {
    "id": "eab40749"
   },
   "source": [
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ëœ ëª¨ë¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096575f3",
   "metadata": {
    "id": "096575f3"
   },
   "outputs": [],
   "source": [
    "# ê°œì„ ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "xgb_improved = XGBClassifier(\n",
    "    n_estimators=200,           # íŠ¸ë¦¬ ê°œìˆ˜ ì¦ê°€\n",
    "    max_depth=6,                # ê¹Šì´ ì¡°ì •\n",
    "    learning_rate=0.1,          # í•™ìŠµë¥  ì¡°ì •\n",
    "    subsample=0.8,              # ì„œë¸Œìƒ˜í”Œë§\n",
    "    colsample_bytree=0.8,       # í”¼ì²˜ ì„œë¸Œìƒ˜í”Œë§\n",
    "    reg_alpha=0.1,              # L1 ì •ê·œí™”\n",
    "    reg_lambda=1.0,             # L2 ì •ê·œí™”\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_improved.fit(X_train_vec, y_train)\n",
    "\n",
    "val_probs_improved = xgb_improved.predict_proba(X_val_vec)[:, 1]\n",
    "auc_improved = roc_auc_score(y_val, val_probs_improved)\n",
    "print(f\"ê°œì„ ëœ ëª¨ë¸ Validation AUC: {auc_improved:.4f}\")\n",
    "print(f\"ì„±ëŠ¥ í–¥ìƒ: {auc_improved - auc_baseline:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a59ffbb",
   "metadata": {
    "id": "2a59ffbb"
   },
   "source": [
    "# êµì°¨ ê²€ì¦ìœ¼ë¡œ ì•ˆì •ì„± í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c799073",
   "metadata": {
    "id": "9c799073"
   },
   "outputs": [],
   "source": [
    "# 5-fold êµì°¨ ê²€ì¦\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# ì „ì²´ í›ˆë ¨ ë°ì´í„°ë¡œ êµì°¨ ê²€ì¦\n",
    "X_full_vec = vectorizer.fit_transform(X)\n",
    "cv_scores = cross_val_score(xgb_improved, X_full_vec, y,\n",
    "                           cv=cv, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "print(f\"êµì°¨ ê²€ì¦ AUC: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}\")\n",
    "print(f\"ê°œë³„ ì ìˆ˜: {cv_scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3e8bfd",
   "metadata": {
    "id": "6e3e8bfd"
   },
   "source": [
    "# ì„±ëŠ¥ ë¹„êµ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8132af",
   "metadata": {
    "id": "3e8132af"
   },
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ\n",
    "models = ['ë² ì´ìŠ¤ë¼ì¸', 'ê°œì„ ëœ ëª¨ë¸']\n",
    "scores = [auc_baseline, auc_improved]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars = plt.bar(models, scores, color=['lightblue', 'orange'], alpha=0.7)\n",
    "plt.ylim(0.85, max(scores) + 0.01)\n",
    "plt.ylabel('Validation AUC')\n",
    "plt.title('ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# ì ìˆ˜ í‘œì‹œ\n",
    "for bar, score in zip(bars, scores):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
    "             f'{score:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72aec57",
   "metadata": {
    "id": "e72aec57"
   },
   "source": [
    "# Inference (ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì‚¬ìš©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f46531",
   "metadata": {
    "id": "25f46531"
   },
   "outputs": [],
   "source": [
    "# ìµœì¢… ëª¨ë¸ë¡œ ì „ì²´ ë°ì´í„° ì¬í›ˆë ¨\n",
    "final_model = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# ì „ì²´ í›ˆë ¨ ë°ì´í„°ë¡œ í•™ìŠµ\n",
    "X_full_vec = vectorizer.fit_transform(X)\n",
    "final_model.fit(X_full_vec, y)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡\n",
    "test = test.rename(columns={'paragraph_text': 'full_text'})\n",
    "X_test = test[['title', 'full_text']]\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "probs = final_model.predict_proba(X_test_vec)[:, 1]\n",
    "print(f\"ì˜ˆì¸¡ ì™„ë£Œ. ì˜ˆì¸¡ê°’ ë²”ìœ„: [{probs.min():.3f}, {probs.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722ab174",
   "metadata": {
    "id": "722ab174"
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75066df6",
   "metadata": {
    "id": "75066df6"
   },
   "outputs": [],
   "source": [
    "# Submission íŒŒì¼ ìƒì„±\n",
    "try:\n",
    "    # sample_submission íŒŒì¼ ì½ê¸° (ì•ˆì „í•˜ê²Œ)\n",
    "    if 'sample_submission' not in locals():\n",
    "        sample_submission = read_csv_robust('sample_submission.csv')\n",
    "    \n",
    "    print(f\"Sample submission shape: {sample_submission.shape}\")\n",
    "    print(f\"Sample submission ì»¬ëŸ¼: {sample_submission.columns.tolist()}\")\n",
    "    \n",
    "    # ì˜ˆì¸¡ê°’ í• ë‹¹\n",
    "    sample_submission['generated'] = probs\n",
    "    \n",
    "    print(f\"ì˜ˆì¸¡ê°’ í†µê³„:\")\n",
    "    print(f\"- ìµœì†Ÿê°’: {probs.min():.4f}\")\n",
    "    print(f\"- ìµœëŒ“ê°’: {probs.max():.4f}\")\n",
    "    print(f\"- í‰ê· ê°’: {probs.mean():.4f}\")\n",
    "    print(f\"- í‘œì¤€í¸ì°¨: {probs.std():.4f}\")\n",
    "    \n",
    "    # íŒŒì¼ ì €ì¥\n",
    "    output_filename = 'improved_submission.csv'\n",
    "    sample_submission.to_csv(output_filename, index=False)\n",
    "    print(f\"âœ… ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_filename}\")\n",
    "    \n",
    "    # íŒŒì¼ ë‹¤ìš´ë¡œë“œ (ì½”ë©ì—ì„œ)\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        files.download(output_filename)\n",
    "        print(f\"âœ… íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {output_filename}\")\n",
    "    except ImportError:\n",
    "        print(\"ë¡œì»¬ í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. íŒŒì¼ì´ í˜„ì¬ ë””ë ‰í„°ë¦¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    # ì„±ëŠ¥ ê°œì„  ìš”ì•½\n",
    "    print(f\"\\n=== ğŸ¯ ì„±ëŠ¥ ê°œì„  ìš”ì•½ ===\")\n",
    "    if 'auc_baseline' in locals() and 'auc_improved' in locals():\n",
    "        print(f\"ë² ì´ìŠ¤ë¼ì¸ AUC: {auc_baseline:.4f}\")\n",
    "        print(f\"ê°œì„ ëœ AUC: {auc_improved:.4f}\")\n",
    "        print(f\"ê°œì„ í­: +{auc_improved - auc_baseline:.4f}\")\n",
    "    if 'cv_scores' in locals():\n",
    "        print(f\"êµì°¨ê²€ì¦ AUC: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}\")\n",
    "    \n",
    "    print(f\"\\nğŸ”¥ ìµœì¢… ëª¨ë¸ ì˜ˆì¸¡ ì™„ë£Œ!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì œì¶œ íŒŒì¼ ìƒì„± ì˜¤ë¥˜: {e}\")\n",
    "    print(\"ì´ì „ ë‹¨ê³„ë“¤ì´ ëª¨ë‘ ì„±ê³µì ìœ¼ë¡œ ì‹¤í–‰ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
