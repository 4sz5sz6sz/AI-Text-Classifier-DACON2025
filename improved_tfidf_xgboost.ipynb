{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e74b97e",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/4sz5sz6sz/AI-Text-Classifier-DACON2025/blob/main/improved_tfidf_xgboost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e16bb3",
   "metadata": {
    "id": "f3e16bb3"
   },
   "source": [
    "# TF-IDF + XGBoost 개선 실험\n",
    "\n",
    "**개선 사항:**\n",
    "- 하이퍼파라미터 튜닝 추가\n",
    "- 교차 검증 적용\n",
    "- 피처 개수 조정\n",
    "- 성능 비교 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b01a364",
   "metadata": {},
   "source": [
    "## 🚀 Google Colab 실행 가이드\n",
    "\n",
    "## 파일 업로드 방법\n",
    "\n",
    "**대용량 파일(500MB+)**: Google Drive 공개 링크 사용 (아래 코드 참고)\n",
    "**소용량 파일**: 직접 업로드\n",
    "\n",
    "**주의사항:**\n",
    "- 파일 업로드는 한 번만 하면 됩니다\n",
    "- 런타임이 재시작되면 다시 업로드해야 합니다\n",
    "- 인코딩 오류가 발생하면 자동으로 여러 방식을 시도합니다\n",
    "\n",
    "**실행 순서:**\n",
    "1. 모든 셀을 순서대로 실행\n",
    "2. 파일 업로드가 요청되면 데이터 파일들을 업로드\n",
    "3. 최종 결과는 자동으로 다운로드됩니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d8caaf",
   "metadata": {
    "id": "69d8caaf"
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcbb6404",
   "metadata": {
    "id": "dcbb6404"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b143f580",
   "metadata": {
    "id": "b143f580"
   },
   "source": [
    "# Data Load & Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4zGT3sBgtRHM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4zGT3sBgtRHM",
    "outputId": "a579bdde-4e89-4e86-9543-32e8c8caa07a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== train.csv 읽기 ===\n",
      "'utf-8-sig' 인코딩으로 시도 중...\n",
      "'utf-8-sig' 인코딩에서 다른 오류: Error tokenizing data. C error: out of memory\n",
      "'utf-8' 인코딩으로 시도 중...\n",
      "'utf-8-sig' 인코딩에서 다른 오류: Error tokenizing data. C error: out of memory\n",
      "'utf-8' 인코딩으로 시도 중...\n",
      "성공! 'utf-8' 인코딩으로 파일을 읽었습니다.\n",
      "Train 데이터 shape: (97172, 3)\n",
      "\n",
      "=== test.csv 읽기 ===\n",
      "'utf-8-sig' 인코딩으로 시도 중...\n",
      "성공! 'utf-8-sig' 인코딩으로 파일을 읽었습니다.\n",
      "Test 데이터 shape: (1962, 4)\n",
      "성공! 'utf-8' 인코딩으로 파일을 읽었습니다.\n",
      "Train 데이터 shape: (97172, 3)\n",
      "\n",
      "=== test.csv 읽기 ===\n",
      "'utf-8-sig' 인코딩으로 시도 중...\n",
      "성공! 'utf-8-sig' 인코딩으로 파일을 읽었습니다.\n",
      "Test 데이터 shape: (1962, 4)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "# 코랩에서 파일 업로드 (처음 실행 시에만)\n",
    "def upload_files_if_needed():\n",
    "    required_files = ['train.csv', 'test.csv', 'sample_submission.csv']\n",
    "    missing_files = [f for f in required_files if not os.path.exists(f)]\n",
    "    \n",
    "    if missing_files:\n",
    "        print(f\"다음 파일들을 업로드해주세요: {missing_files}\")\n",
    "        uploaded = files.upload()\n",
    "        print(\"파일 업로드 완료!\")\n",
    "        return uploaded\n",
    "    else:\n",
    "        print(\"모든 필요한 파일이 이미 존재합니다.\")\n",
    "        return None\n",
    "\n",
    "# 강화된 CSV 읽기 함수\n",
    "def read_csv_robust(file_path):\n",
    "    \"\"\"다양한 방법으로 CSV 파일을 읽어보는 함수\"\"\"\n",
    "    \n",
    "    # 방법 1: 다양한 인코딩 시도\n",
    "    encodings = ['utf-8', 'utf-8-sig', 'cp949', 'euc-kr', 'latin-1', 'iso-8859-1']\n",
    "    \n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            print(f\"'{encoding}' 인코딩으로 시도 중...\")\n",
    "            df = pd.read_csv(file_path, encoding=encoding)\n",
    "            print(f\"✅ 성공! '{encoding}' 인코딩으로 파일을 읽었습니다.\")\n",
    "            return df\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"❌ '{encoding}' 인코딩 실패\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"❌ '{encoding}' 인코딩에서 오류: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # 방법 2: 바이너리 모드로 읽어서 인코딩 감지\n",
    "    try:\n",
    "        print(\"인코딩 자동 감지를 시도합니다...\")\n",
    "        import chardet\n",
    "        \n",
    "        with open(file_path, 'rb') as f:\n",
    "            raw_data = f.read()\n",
    "            result = chardet.detect(raw_data)\n",
    "            encoding = result['encoding']\n",
    "            confidence = result['confidence']\n",
    "            \n",
    "        print(f\"감지된 인코딩: {encoding} (신뢰도: {confidence:.2f})\")\n",
    "        \n",
    "        if confidence > 0.7:  # 신뢰도가 70% 이상인 경우만\n",
    "            df = pd.read_csv(file_path, encoding=encoding)\n",
    "            print(f\"✅ 자동 감지된 인코딩으로 성공!\")\n",
    "            return df\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"chardet 라이브러리가 없습니다. 설치를 시도합니다...\")\n",
    "        !pip install chardet\n",
    "        import chardet\n",
    "        # 위 코드 재실행\n",
    "        with open(file_path, 'rb') as f:\n",
    "            raw_data = f.read()\n",
    "            result = chardet.detect(raw_data)\n",
    "            encoding = result['encoding']\n",
    "            \n",
    "        df = pd.read_csv(file_path, encoding=encoding)\n",
    "        print(f\"✅ chardet 설치 후 성공!\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 자동 감지 실패: {e}\")\n",
    "    \n",
    "    # 방법 3: 에러 처리 옵션 사용\n",
    "    try:\n",
    "        print(\"에러 무시 모드로 시도합니다...\")\n",
    "        df = pd.read_csv(file_path, encoding='utf-8', errors='ignore')\n",
    "        print(\"✅ 에러 무시 모드로 성공!\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 에러 무시 모드 실패: {e}\")\n",
    "    \n",
    "    raise Exception(\"❌ 모든 방법으로 파일을 읽는데 실패했습니다.\")\n",
    "\n",
    "# 파일 업로드 (필요한 경우)\n",
    "uploaded = upload_files_if_needed()\n",
    "\n",
    "# 파일 읽기\n",
    "try:\n",
    "    print(\"\\n=== train.csv 읽기 ===\")\n",
    "    train = read_csv_robust('train.csv')\n",
    "    print(f\"Train 데이터 shape: {train.shape}\")\n",
    "    print(f\"Train 컬럼: {train.columns.tolist()}\")\n",
    "\n",
    "    print(\"\\n=== test.csv 읽기 ===\")\n",
    "    test = read_csv_robust('test.csv')\n",
    "    print(f\"Test 데이터 shape: {test.shape}\")\n",
    "    print(f\"Test 컬럼: {test.columns.tolist()}\")\n",
    "\n",
    "    print(\"\\n=== sample_submission.csv 읽기 ===\")\n",
    "    sample_submission = read_csv_robust('sample_submission.csv')\n",
    "    print(f\"Sample submission shape: {sample_submission.shape}\")\n",
    "    print(f\"Sample submission 컬럼: {sample_submission.columns.tolist()}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ 파일 읽기 실패: {e}\")\n",
    "    print(\"다음을 확인해주세요:\")\n",
    "    print(\"1. 파일이 올바르게 업로드되었는지\")\n",
    "    print(\"2. 파일명이 정확한지 (train.csv, test.csv, sample_submission.csv)\")\n",
    "    print(\"3. 파일이 손상되지 않았는지\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee25cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대용량 파일 다운로드 (Google Drive 공개 링크)\n",
    "import gdown\n",
    "\n",
    "# 파일 ID 설정 (Google Drive 링크에서 추출)\n",
    "TRAIN_FILE_ID = \"1teA9GmYlIsutaDLWvCCsLeh7833t-TC_\"  # 제공받은 train.csv ID\n",
    "TEST_FILE_ID = \"\"     # test.csv ID (필요시 입력)\n",
    "SAMPLE_FILE_ID = \"\"   # sample_submission.csv ID (필요시 입력)\n",
    "\n",
    "def download_from_drive(file_id, filename):\n",
    "    if file_id:\n",
    "        url = f'https://drive.google.com/uc?id={file_id}'\n",
    "        gdown.download(url, filename, quiet=False)\n",
    "        print(f\"✅ {filename} 다운로드 완료!\")\n",
    "\n",
    "# 파일 다운로드 실행\n",
    "if TRAIN_FILE_ID and not os.path.exists('train.csv'):\n",
    "    print(\"📥 train.csv 다운로드 중...\")\n",
    "    download_from_drive(TRAIN_FILE_ID, 'train.csv')\n",
    "    \n",
    "if TEST_FILE_ID and not os.path.exists('test.csv'):\n",
    "    download_from_drive(TEST_FILE_ID, 'test.csv')\n",
    "    \n",
    "if SAMPLE_FILE_ID and not os.path.exists('sample_submission.csv'):\n",
    "    download_from_drive(SAMPLE_FILE_ID, 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b411202",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "6b411202",
    "outputId": "cbee5210-f6fd-4775-e951-e87ade0ecadb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (97172, 3)\n",
      "Test shape: (1962, 4)\n",
      "Generated 비율: 0.082\n"
     ]
    }
   ],
   "source": [
    "#train = pd.read_csv('./train.csv', encoding='utf-8-sig')\n",
    "#test = pd.read_csv('./test.csv', encoding='utf-8-sig')\n",
    "\n",
    "# 데이터 확인\n",
    "try:\n",
    "    print(\"=== 데이터 기본 정보 ===\")\n",
    "    print(f\"Train shape: {train.shape}\")\n",
    "    print(f\"Test shape: {test.shape}\")\n",
    "    \n",
    "    # 컬럼 확인\n",
    "    print(f\"\\nTrain 컬럼: {train.columns.tolist()}\")\n",
    "    print(f\"Test 컬럼: {test.columns.tolist()}\")\n",
    "    \n",
    "    # target 변수 확인\n",
    "    if 'generated' in train.columns:\n",
    "        print(f\"\\nGenerated 비율: {train['generated'].mean():.3f}\")\n",
    "        print(f\"Generated 값 분포:\")\n",
    "        print(train['generated'].value_counts())\n",
    "    else:\n",
    "        print(\"\\n⚠️ 'generated' 컬럼을 찾을 수 없습니다.\")\n",
    "        print(\"사용 가능한 컬럼:\", train.columns.tolist())\n",
    "    \n",
    "    # 데이터 미리보기\n",
    "    print(\"\\n=== Train 데이터 미리보기 ===\")\n",
    "    print(train.head())\n",
    "    \n",
    "    print(\"\\n=== Test 데이터 미리보기 ===\")\n",
    "    print(test.head())\n",
    "    \n",
    "    # 결측값 확인\n",
    "    print(\"\\n=== 결측값 확인 ===\")\n",
    "    print(\"Train 결측값:\")\n",
    "    print(train.isnull().sum())\n",
    "    print(\"\\nTest 결측값:\")\n",
    "    print(test.isnull().sum())\n",
    "    \n",
    "except NameError:\n",
    "    print(\"❌ 데이터가 로드되지 않았습니다. 위의 셀을 먼저 실행해주세요.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 데이터 확인 중 오류: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9c6724",
   "metadata": {
    "id": "cf9c6724"
   },
   "outputs": [],
   "source": [
    "# 데이터 전처리 및 분할\n",
    "try:\n",
    "    # 테스트 데이터 컬럼명 통일 (필요한 경우)\n",
    "    if 'paragraph_text' in test.columns and 'full_text' not in test.columns:\n",
    "        test = test.rename(columns={'paragraph_text': 'full_text'})\n",
    "        print(\"✅ Test 데이터의 'paragraph_text' 컬럼을 'full_text'로 변경했습니다.\")\n",
    "    \n",
    "    # 필요한 컬럼 확인\n",
    "    required_train_cols = ['title', 'full_text', 'generated']\n",
    "    required_test_cols = ['title', 'full_text']\n",
    "    \n",
    "    missing_train_cols = [col for col in required_train_cols if col not in train.columns]\n",
    "    missing_test_cols = [col for col in required_test_cols if col not in test.columns]\n",
    "    \n",
    "    if missing_train_cols:\n",
    "        print(f\"❌ Train 데이터에서 누락된 컬럼: {missing_train_cols}\")\n",
    "        print(f\"사용 가능한 컬럼: {train.columns.tolist()}\")\n",
    "        raise ValueError(\"필요한 컬럼이 누락되었습니다.\")\n",
    "    \n",
    "    if missing_test_cols:\n",
    "        print(f\"❌ Test 데이터에서 누락된 컬럼: {missing_test_cols}\")\n",
    "        print(f\"사용 가능한 컬럼: {test.columns.tolist()}\")\n",
    "        raise ValueError(\"필요한 컬럼이 누락되었습니다.\")\n",
    "    \n",
    "    # 데이터 분할\n",
    "    X = train[['title', 'full_text']]\n",
    "    y = train['generated']\n",
    "    \n",
    "    print(f\"✅ 피처 데이터 shape: {X.shape}\")\n",
    "    print(f\"✅ 타겟 데이터 shape: {y.shape}\")\n",
    "    \n",
    "    # Train-Validation 분할\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, stratify=y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ 훈련 세트: {X_train.shape}\")\n",
    "    print(f\"✅ 검증 세트: {X_val.shape}\")\n",
    "    print(f\"✅ 훈련 세트 타겟 비율: {y_train.mean():.3f}\")\n",
    "    print(f\"✅ 검증 세트 타겟 비율: {y_val.mean():.3f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 데이터 전처리 오류: {e}\")\n",
    "    print(\"데이터 구조를 다시 확인해주세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc44e7d",
   "metadata": {
    "id": "fbc44e7d"
   },
   "source": [
    "# 개선된 TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3c9cafe",
   "metadata": {
    "id": "f3c9cafe"
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      6\u001b[39m vectorizer = FeatureUnion([\n\u001b[32m      7\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m, Pipeline([(\u001b[33m'\u001b[39m\u001b[33mselector\u001b[39m\u001b[33m'\u001b[39m, get_title),\n\u001b[32m      8\u001b[39m                         (\u001b[33m'\u001b[39m\u001b[33mtfidf\u001b[39m\u001b[33m'\u001b[39m, TfidfVectorizer(ngram_range=(\u001b[32m1\u001b[39m,\u001b[32m3\u001b[39m), max_features=\u001b[32m5000\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m                                                     min_df=\u001b[32m2\u001b[39m, max_df=\u001b[32m0.95\u001b[39m))])),\n\u001b[32m     13\u001b[39m ])\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# 피처 변환\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m X_train_vec = \u001b[43mvectorizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m X_val_vec = vectorizer.transform(X_val)\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m피처 차원: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train_vec.shape[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    322\u001b[39m         return_tuple = (\n\u001b[32m    323\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    324\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    325\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\pipeline.py:1974\u001b[39m, in \u001b[36mFeatureUnion.fit_transform\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1971\u001b[39m             routed_params[name] = Bunch(transform={})\n\u001b[32m   1972\u001b[39m             routed_params[name].fit = params\n\u001b[32m-> \u001b[39m\u001b[32m1974\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parallel_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_fit_transform_one\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1975\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m results:\n\u001b[32m   1976\u001b[39m     \u001b[38;5;66;03m# All transformers are None\u001b[39;00m\n\u001b[32m   1977\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.zeros((X.shape[\u001b[32m0\u001b[39m], \u001b[32m0\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\pipeline.py:1996\u001b[39m, in \u001b[36mFeatureUnion._parallel_func\u001b[39m\u001b[34m(self, X, y, func, routed_params)\u001b[39m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_transformer_weights()\n\u001b[32m   1994\u001b[39m transformers = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m._iter())\n\u001b[32m-> \u001b[39m\u001b[32m1996\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1997\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1998\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2000\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2001\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFeatureUnion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtransformers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2005\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2006\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtransformers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2007\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1916\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1917\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1918\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1920\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1921\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1922\u001b[39m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1923\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1924\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1925\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1845\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1846\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1847\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1848\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1849\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:139\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m     config = {}\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config):\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\pipeline.py:1551\u001b[39m, in \u001b[36m_fit_transform_one\u001b[39m\u001b[34m(transformer, X, y, weight, message_clsname, message, params)\u001b[39m\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[32m   1550\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[33m\"\u001b[39m\u001b[33mfit_transform\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1551\u001b[39m         res = \u001b[43mtransformer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit_transform\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1552\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1553\u001b[39m         res = transformer.fit(X, y, **params.get(\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m, {})).transform(\n\u001b[32m   1554\u001b[39m             X, **params.get(\u001b[33m\"\u001b[39m\u001b[33mtransform\u001b[39m\u001b[33m\"\u001b[39m, {})\n\u001b[32m   1555\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\pipeline.py:730\u001b[39m, in \u001b[36mPipeline.fit_transform\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    724\u001b[39m last_step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    725\u001b[39m     step_idx=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) - \u001b[32m1\u001b[39m,\n\u001b[32m    726\u001b[39m     step_params=routed_params[\u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]],\n\u001b[32m    727\u001b[39m     all_params=params,\n\u001b[32m    728\u001b[39m )\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(last_step, \u001b[33m\"\u001b[39m\u001b[33mfit_transform\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m730\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlast_step\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m        \u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit_transform\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m last_step.fit(Xt, y, **last_step_params[\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m]).transform(\n\u001b[32m    735\u001b[39m         Xt, **last_step_params[\u001b[33m\"\u001b[39m\u001b[33mtransform\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    736\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2104\u001b[39m, in \u001b[36mTfidfVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   2097\u001b[39m \u001b[38;5;28mself\u001b[39m._check_params()\n\u001b[32m   2098\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf = TfidfTransformer(\n\u001b[32m   2099\u001b[39m     norm=\u001b[38;5;28mself\u001b[39m.norm,\n\u001b[32m   2100\u001b[39m     use_idf=\u001b[38;5;28mself\u001b[39m.use_idf,\n\u001b[32m   2101\u001b[39m     smooth_idf=\u001b[38;5;28mself\u001b[39m.smooth_idf,\n\u001b[32m   2102\u001b[39m     sublinear_tf=\u001b[38;5;28mself\u001b[39m.sublinear_tf,\n\u001b[32m   2103\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2104\u001b[39m X = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2105\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf.fit(X)\n\u001b[32m   2106\u001b[39m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[32m   2107\u001b[39m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1376\u001b[39m, in \u001b[36mCountVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   1368\u001b[39m             warnings.warn(\n\u001b[32m   1369\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mUpper case characters found in\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1370\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m vocabulary while \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlowercase\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1371\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m is True. These entries will not\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1372\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m be matched with any documents\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1373\u001b[39m             )\n\u001b[32m   1374\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1376\u001b[39m vocabulary, X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.binary:\n\u001b[32m   1379\u001b[39m     X.data.fill(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1265\u001b[39m, in \u001b[36mCountVectorizer._count_vocab\u001b[39m\u001b[34m(self, raw_documents, fixed_vocab)\u001b[39m\n\u001b[32m   1263\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m analyze(doc):\n\u001b[32m   1264\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1265\u001b[39m         feature_idx = \u001b[43mvocabulary\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   1266\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m feature_idx \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m feature_counter:\n\u001b[32m   1267\u001b[39m             feature_counter[feature_idx] = \u001b[32m1\u001b[39m\n",
      "\u001b[31mMemoryError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 피처 개수 늘리고 다양한 n-gram 시도\n",
    "get_title = FunctionTransformer(lambda x: x['title'], validate=False)\n",
    "get_text = FunctionTransformer(lambda x: x['full_text'], validate=False)\n",
    "\n",
    "# 메모리 효율적인 벡터화 (피처 개수 조정)\n",
    "vectorizer = FeatureUnion([\n",
    "    ('title', Pipeline([('selector', get_title),\n",
    "                        ('tfidf', TfidfVectorizer(ngram_range=(1,2), max_features=2000,\n",
    "                                                min_df=3, max_df=0.9))])),\n",
    "    ('full_text', Pipeline([('selector', get_text),\n",
    "                            ('tfidf', TfidfVectorizer(ngram_range=(1,2), max_features=8000,\n",
    "                                                    min_df=3, max_df=0.9))])),\n",
    "])\n",
    "\n",
    "# 피처 변환\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_val_vec = vectorizer.transform(X_val)\n",
    "\n",
    "print(f\"피처 차원: {X_train_vec.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf606fd",
   "metadata": {
    "id": "7bf606fd"
   },
   "source": [
    "# 베이스라인 모델 (비교용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488bae9f",
   "metadata": {
    "id": "488bae9f"
   },
   "outputs": [],
   "source": [
    "# 기본 XGBoost (베이스라인)\n",
    "xgb_baseline = XGBClassifier(random_state=42)\n",
    "xgb_baseline.fit(X_train_vec, y_train)\n",
    "\n",
    "val_probs_baseline = xgb_baseline.predict_proba(X_val_vec)[:, 1]\n",
    "auc_baseline = roc_auc_score(y_val, val_probs_baseline)\n",
    "print(f\"베이스라인 Validation AUC: {auc_baseline:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab40749",
   "metadata": {
    "id": "eab40749"
   },
   "source": [
    "# 하이퍼파라미터 튜닝된 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096575f3",
   "metadata": {
    "id": "096575f3"
   },
   "outputs": [],
   "source": [
    "# 개선된 하이퍼파라미터\n",
    "xgb_improved = XGBClassifier(\n",
    "    n_estimators=200,           # 트리 개수 증가\n",
    "    max_depth=6,                # 깊이 조정\n",
    "    learning_rate=0.1,          # 학습률 조정\n",
    "    subsample=0.8,              # 서브샘플링\n",
    "    colsample_bytree=0.8,       # 피처 서브샘플링\n",
    "    reg_alpha=0.1,              # L1 정규화\n",
    "    reg_lambda=1.0,             # L2 정규화\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_improved.fit(X_train_vec, y_train)\n",
    "\n",
    "val_probs_improved = xgb_improved.predict_proba(X_val_vec)[:, 1]\n",
    "auc_improved = roc_auc_score(y_val, val_probs_improved)\n",
    "print(f\"개선된 모델 Validation AUC: {auc_improved:.4f}\")\n",
    "print(f\"성능 향상: {auc_improved - auc_baseline:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a59ffbb",
   "metadata": {
    "id": "2a59ffbb"
   },
   "source": [
    "# 교차 검증으로 안정성 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c799073",
   "metadata": {
    "id": "9c799073"
   },
   "outputs": [],
   "source": [
    "# 5-fold 교차 검증\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 전체 훈련 데이터로 교차 검증\n",
    "X_full_vec = vectorizer.fit_transform(X)\n",
    "cv_scores = cross_val_score(xgb_improved, X_full_vec, y,\n",
    "                           cv=cv, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "print(f\"교차 검증 AUC: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "print(f\"개별 점수: {cv_scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3e8bfd",
   "metadata": {
    "id": "6e3e8bfd"
   },
   "source": [
    "# 성능 비교 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8132af",
   "metadata": {
    "id": "3e8132af"
   },
   "outputs": [],
   "source": [
    "# 모델 성능 비교\n",
    "models = ['베이스라인', '개선된 모델']\n",
    "scores = [auc_baseline, auc_improved]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars = plt.bar(models, scores, color=['lightblue', 'orange'], alpha=0.7)\n",
    "plt.ylim(0.85, max(scores) + 0.01)\n",
    "plt.ylabel('Validation AUC')\n",
    "plt.title('모델 성능 비교')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 점수 표시\n",
    "for bar, score in zip(bars, scores):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
    "             f'{score:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72aec57",
   "metadata": {
    "id": "e72aec57"
   },
   "source": [
    "# Inference (최고 성능 모델 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f46531",
   "metadata": {
    "id": "25f46531"
   },
   "outputs": [],
   "source": [
    "# 최종 모델로 전체 데이터 재훈련\n",
    "final_model = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 전체 훈련 데이터로 학습\n",
    "X_full_vec = vectorizer.fit_transform(X)\n",
    "final_model.fit(X_full_vec, y)\n",
    "\n",
    "# 테스트 데이터 예측\n",
    "test = test.rename(columns={'paragraph_text': 'full_text'})\n",
    "X_test = test[['title', 'full_text']]\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "probs = final_model.predict_proba(X_test_vec)[:, 1]\n",
    "print(f\"예측 완료. 예측값 범위: [{probs.min():.3f}, {probs.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722ab174",
   "metadata": {
    "id": "722ab174"
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75066df6",
   "metadata": {
    "id": "75066df6"
   },
   "outputs": [],
   "source": [
    "# Submission 파일 생성\n",
    "try:\n",
    "    # sample_submission 파일 읽기 (안전하게)\n",
    "    if 'sample_submission' not in locals():\n",
    "        sample_submission = read_csv_robust('sample_submission.csv')\n",
    "    \n",
    "    print(f\"Sample submission shape: {sample_submission.shape}\")\n",
    "    print(f\"Sample submission 컬럼: {sample_submission.columns.tolist()}\")\n",
    "    \n",
    "    # 예측값 할당\n",
    "    sample_submission['generated'] = probs\n",
    "    \n",
    "    print(f\"예측값 통계:\")\n",
    "    print(f\"- 최솟값: {probs.min():.4f}\")\n",
    "    print(f\"- 최댓값: {probs.max():.4f}\")\n",
    "    print(f\"- 평균값: {probs.mean():.4f}\")\n",
    "    print(f\"- 표준편차: {probs.std():.4f}\")\n",
    "    \n",
    "    # 파일 저장\n",
    "    output_filename = 'improved_submission.csv'\n",
    "    sample_submission.to_csv(output_filename, index=False)\n",
    "    print(f\"✅ 제출 파일 저장 완료: {output_filename}\")\n",
    "    \n",
    "    # 파일 다운로드 (코랩에서)\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        files.download(output_filename)\n",
    "        print(f\"✅ 파일 다운로드 시작: {output_filename}\")\n",
    "    except ImportError:\n",
    "        print(\"로컬 환경에서 실행 중입니다. 파일이 현재 디렉터리에 저장되었습니다.\")\n",
    "    \n",
    "    # 성능 개선 요약\n",
    "    print(f\"\\n=== 🎯 성능 개선 요약 ===\")\n",
    "    if 'auc_baseline' in locals() and 'auc_improved' in locals():\n",
    "        print(f\"베이스라인 AUC: {auc_baseline:.4f}\")\n",
    "        print(f\"개선된 AUC: {auc_improved:.4f}\")\n",
    "        print(f\"개선폭: +{auc_improved - auc_baseline:.4f}\")\n",
    "    if 'cv_scores' in locals():\n",
    "        print(f\"교차검증 AUC: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "    \n",
    "    print(f\"\\n🔥 최종 모델 예측 완료!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 제출 파일 생성 오류: {e}\")\n",
    "    print(\"이전 단계들이 모두 성공적으로 실행되었는지 확인해주세요.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
