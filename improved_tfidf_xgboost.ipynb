{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e74b97e",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/4sz5sz6sz/AI-Text-Classifier-DACON2025/blob/main/improved_tfidf_xgboost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e16bb3",
   "metadata": {
    "id": "f3e16bb3"
   },
   "source": [
    "# TF-IDF + XGBoost ê°œì„  ì‹¤í—˜\n",
    "\n",
    "**ê°œì„  ì‚¬í•­:**\n",
    "- í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì¶”ê°€\n",
    "- êµì°¨ ê²€ì¦ ì ìš©\n",
    "- í”¼ì²˜ ê°œìˆ˜ ì¡°ì •\n",
    "- ì„±ëŠ¥ ë¹„êµ ë¶„ì„"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b01a364",
   "metadata": {},
   "source": [
    "## ğŸš€ Google Colab ì‹¤í–‰ ê°€ì´ë“œ\n",
    "\n",
    "## íŒŒì¼ ì—…ë¡œë“œ ë°©ë²•\n",
    "\n",
    "**ëŒ€ìš©ëŸ‰ íŒŒì¼(500MB+)**: Google Drive ê³µê°œ ë§í¬ ì‚¬ìš© (ì•„ë˜ ì½”ë“œ ì°¸ê³ )\n",
    "**ì†Œìš©ëŸ‰ íŒŒì¼**: ì§ì ‘ ì—…ë¡œë“œ\n",
    "\n",
    "**ì£¼ì˜ì‚¬í•­:**\n",
    "- íŒŒì¼ ì—…ë¡œë“œëŠ” í•œ ë²ˆë§Œ í•˜ë©´ ë©ë‹ˆë‹¤\n",
    "- ëŸ°íƒ€ì„ì´ ì¬ì‹œì‘ë˜ë©´ ë‹¤ì‹œ ì—…ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤\n",
    "- ì¸ì½”ë”© ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ ìë™ìœ¼ë¡œ ì—¬ëŸ¬ ë°©ì‹ì„ ì‹œë„í•©ë‹ˆë‹¤\n",
    "\n",
    "**ì‹¤í–‰ ìˆœì„œ:**\n",
    "1. ëª¨ë“  ì…€ì„ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰\n",
    "2. íŒŒì¼ ì—…ë¡œë“œê°€ ìš”ì²­ë˜ë©´ ë°ì´í„° íŒŒì¼ë“¤ì„ ì—…ë¡œë“œ\n",
    "3. ìµœì¢… ê²°ê³¼ëŠ” ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œë©ë‹ˆë‹¤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d8caaf",
   "metadata": {
    "id": "69d8caaf"
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcbb6404",
   "metadata": {
    "id": "dcbb6404"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b143f580",
   "metadata": {
    "id": "b143f580"
   },
   "source": [
    "# Data Load & Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf0105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”— ëŒ€ìš©ëŸ‰ íŒŒì¼ ìë™ ë‹¤ìš´ë¡œë“œ\n",
    "import gdown\n",
    "import os\n",
    "\n",
    "# íŒŒì¼ ID ì„¤ì • (Google Drive ê³µê°œ ë§í¬ì—ì„œ ì¶”ì¶œ)\n",
    "TRAIN_FILE_ID = \"1teA9GmYlIsutaDLWvCCsLeh7833t-TC_\"\n",
    "TEST_FILE_ID = \"1bGC_YWtNUOroHARfmzCjrL8oPcvb7Tpw\"\n",
    "SAMPLE_FILE_ID = \"1ebrHVj-CtM-7aEz4OqP-bCHlazle-PKM\"\n",
    "\n",
    "def download_from_drive(file_id, filename):\n",
    "    if file_id and not os.path.exists(filename):\n",
    "        url = f'https://drive.google.com/uc?id={file_id}'\n",
    "        print(f\"ğŸ“¥ {filename} ë‹¤ìš´ë¡œë“œ ì¤‘...\")\n",
    "        gdown.download(url, filename, quiet=False)\n",
    "        print(f\"âœ… {filename} ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!\")\n",
    "    elif os.path.exists(filename):\n",
    "        print(f\"âœ… {filename} ì´ë¯¸ ì¡´ì¬í•¨\")\n",
    "\n",
    "# ìë™ ë‹¤ìš´ë¡œë“œ ì‹¤í–‰\n",
    "download_from_drive(TRAIN_FILE_ID, 'train.csv')\n",
    "download_from_drive(TEST_FILE_ID, 'test.csv')\n",
    "download_from_drive(SAMPLE_FILE_ID, 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4zGT3sBgtRHM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4zGT3sBgtRHM",
    "outputId": "a579bdde-4e89-4e86-9543-32e8c8caa07a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== train.csv ì½ê¸° ===\n",
      "'utf-8-sig' ì¸ì½”ë”©ìœ¼ë¡œ ì‹œë„ ì¤‘...\n",
      "'utf-8-sig' ì¸ì½”ë”©ì—ì„œ ë‹¤ë¥¸ ì˜¤ë¥˜: Error tokenizing data. C error: out of memory\n",
      "'utf-8' ì¸ì½”ë”©ìœ¼ë¡œ ì‹œë„ ì¤‘...\n",
      "'utf-8-sig' ì¸ì½”ë”©ì—ì„œ ë‹¤ë¥¸ ì˜¤ë¥˜: Error tokenizing data. C error: out of memory\n",
      "'utf-8' ì¸ì½”ë”©ìœ¼ë¡œ ì‹œë„ ì¤‘...\n",
      "ì„±ê³µ! 'utf-8' ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ì„ ì½ì—ˆìŠµë‹ˆë‹¤.\n",
      "Train ë°ì´í„° shape: (97172, 3)\n",
      "\n",
      "=== test.csv ì½ê¸° ===\n",
      "'utf-8-sig' ì¸ì½”ë”©ìœ¼ë¡œ ì‹œë„ ì¤‘...\n",
      "ì„±ê³µ! 'utf-8-sig' ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ì„ ì½ì—ˆìŠµë‹ˆë‹¤.\n",
      "Test ë°ì´í„° shape: (1962, 4)\n",
      "ì„±ê³µ! 'utf-8' ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ì„ ì½ì—ˆìŠµë‹ˆë‹¤.\n",
      "Train ë°ì´í„° shape: (97172, 3)\n",
      "\n",
      "=== test.csv ì½ê¸° ===\n",
      "'utf-8-sig' ì¸ì½”ë”©ìœ¼ë¡œ ì‹œë„ ì¤‘...\n",
      "ì„±ê³µ! 'utf-8-sig' ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ì„ ì½ì—ˆìŠµë‹ˆë‹¤.\n",
      "Test ë°ì´í„° shape: (1962, 4)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import psutil\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸ í•¨ìˆ˜\n",
    "def get_memory_usage():\n",
    "    \"\"\"í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "    return memory_mb\n",
    "\n",
    "print(f\"ğŸ” ì‹œì‘ ì‹œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {get_memory_usage():.1f}MB\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ CSV ì½ê¸° í•¨ìˆ˜\n",
    "def read_csv_robust(file_path):\n",
    "    \"\"\"ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì´ê³  ë‹¤ì–‘í•œ ì¸ì½”ë”©ìœ¼ë¡œ CSV íŒŒì¼ì„ ì½ëŠ” í•¨ìˆ˜\"\"\"\n",
    "    encodings = ['utf-8', 'utf-8-sig', 'cp949', 'euc-kr']\n",
    "    \n",
    "    print(f\"ğŸ“‚ {file_path} ë¡œë”© ì¤‘...\")\n",
    "    start_memory = get_memory_usage()\n",
    "    \n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            # ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë¡œë”© ì„¤ì •\n",
    "            df = pd.read_csv(\n",
    "                file_path, \n",
    "                encoding=encoding,\n",
    "                dtype={'id': 'int32'},  # IDëŠ” int32ë¡œ ì¶©ë¶„\n",
    "                low_memory=False        # íƒ€ì… ì¶”ë¡  ê°œì„ \n",
    "            )\n",
    "            \n",
    "            end_memory = get_memory_usage()\n",
    "            memory_used = end_memory - start_memory\n",
    "            \n",
    "            print(f\"âœ… {file_path} ì½ê¸° ì„±ê³µ\")\n",
    "            print(f\"   - ì¸ì½”ë”©: {encoding}\")\n",
    "            print(f\"   - ë©”ëª¨ë¦¬ ì‚¬ìš©: +{memory_used:.1f}MB\")\n",
    "            print(f\"   - ë°ì´í„° í¬ê¸°: {df.shape}\")\n",
    "            \n",
    "            return df\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    # ëª¨ë“  ì¸ì½”ë”© ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ë¬´ì‹œ ëª¨ë“œ\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding='utf-8', errors='ignore', low_memory=False)\n",
    "        end_memory = get_memory_usage()\n",
    "        memory_used = end_memory - start_memory\n",
    "        \n",
    "        print(f\"âœ… {file_path} ì½ê¸° ì„±ê³µ (ì—ëŸ¬ ë¬´ì‹œ ëª¨ë“œ)\")\n",
    "        print(f\"   - ë©”ëª¨ë¦¬ ì‚¬ìš©: +{memory_used:.1f}MB\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {file_path} ì½ê¸° ì‹¤íŒ¨: {e}\")\n",
    "        raise\n",
    "\n",
    "# ë°ì´í„° íŒŒì¼ ì½ê¸°\n",
    "print(\"ğŸ“ ë°ì´í„° íŒŒì¼ ë¡œë”© ì‹œì‘...\")\n",
    "gc.collect()  # ì‚¬ì „ ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "\n",
    "try:\n",
    "    train = read_csv_robust('train.csv')\n",
    "    test = read_csv_robust('test.csv')\n",
    "    sample_submission = read_csv_robust('sample_submission.csv')\n",
    "    \n",
    "    print(f\"\\nğŸ“Š ë°ì´í„° ë¡œë”© ì™„ë£Œ!\")\n",
    "    print(f\"âœ… Train: {train.shape}\")\n",
    "    print(f\"âœ… Test: {test.shape}\")\n",
    "    print(f\"âœ… Sample submission: {sample_submission.shape}\")\n",
    "    print(f\"ğŸ§  ì´ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {get_memory_usage():.1f}MB\")\n",
    "    \n",
    "    # ë°ì´í„° íƒ€ì… ìµœì í™” (ë©”ëª¨ë¦¬ ì ˆì•½)\n",
    "    for df_name, df in [('train', train), ('test', test)]:\n",
    "        if 'id' in df.columns:\n",
    "            df['id'] = df['id'].astype('int32')\n",
    "        print(f\"ğŸ“ {df_name} ë°ì´í„° íƒ€ì… ìµœì í™” ì™„ë£Œ\")\n",
    "    \n",
    "    gc.collect()\n",
    "    print(\"ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ë°ì´í„° ë¡œë”© ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "    print(\"ğŸ’¡ í•´ê²°ì±…:\")\n",
    "    print(\"1. íŒŒì¼ì´ ì˜¬ë°”ë¥´ê²Œ ë‹¤ìš´ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸\")\n",
    "    print(\"2. ëŸ°íƒ€ì„ ì¬ì‹œì‘ í›„ ë‹¤ì‹œ ì‹œë„\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b411202",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "6b411202",
    "outputId": "cbee5210-f6fd-4775-e951-e87ade0ecadb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (97172, 3)\n",
      "Test shape: (1962, 4)\n",
      "Generated ë¹„ìœ¨: 0.082\n"
     ]
    }
   ],
   "source": [
    "#train = pd.read_csv('./train.csv', encoding='utf-8-sig')\n",
    "#test = pd.read_csv('./test.csv', encoding='utf-8-sig')\n",
    "\n",
    "# ë°ì´í„° í™•ì¸\n",
    "try:\n",
    "    print(\"=== ë°ì´í„° ê¸°ë³¸ ì •ë³´ ===\")\n",
    "    print(f\"Train shape: {train.shape}\")\n",
    "    print(f\"Test shape: {test.shape}\")\n",
    "    \n",
    "    # ì»¬ëŸ¼ í™•ì¸\n",
    "    print(f\"\\nTrain ì»¬ëŸ¼: {train.columns.tolist()}\")\n",
    "    print(f\"Test ì»¬ëŸ¼: {test.columns.tolist()}\")\n",
    "    \n",
    "    # target ë³€ìˆ˜ í™•ì¸\n",
    "    if 'generated' in train.columns:\n",
    "        print(f\"\\nGenerated ë¹„ìœ¨: {train['generated'].mean():.3f}\")\n",
    "        print(f\"Generated ê°’ ë¶„í¬:\")\n",
    "        print(train['generated'].value_counts())\n",
    "    else:\n",
    "        print(\"\\nâš ï¸ 'generated' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        print(\"ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼:\", train.columns.tolist())\n",
    "    \n",
    "    # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°\n",
    "    print(\"\\n=== Train ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° ===\")\n",
    "    print(train.head())\n",
    "    \n",
    "    print(\"\\n=== Test ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° ===\")\n",
    "    print(test.head())\n",
    "    \n",
    "    # ê²°ì¸¡ê°’ í™•ì¸\n",
    "    print(\"\\n=== ê²°ì¸¡ê°’ í™•ì¸ ===\")\n",
    "    print(\"Train ê²°ì¸¡ê°’:\")\n",
    "    print(train.isnull().sum())\n",
    "    print(\"\\nTest ê²°ì¸¡ê°’:\")\n",
    "    print(test.isnull().sum())\n",
    "    \n",
    "except NameError:\n",
    "    print(\"âŒ ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìœ„ì˜ ì…€ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ë°ì´í„° í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9c6724",
   "metadata": {
    "id": "cf9c6724"
   },
   "outputs": [],
   "source": [
    "# ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¶„í• \n",
    "try:\n",
    "    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì»¬ëŸ¼ëª… í†µì¼ (í•„ìš”í•œ ê²½ìš°)\n",
    "    if 'paragraph_text' in test.columns and 'full_text' not in test.columns:\n",
    "        test = test.rename(columns={'paragraph_text': 'full_text'})\n",
    "        print(\"âœ… Test ë°ì´í„°ì˜ 'paragraph_text' ì»¬ëŸ¼ì„ 'full_text'ë¡œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    # í•„ìš”í•œ ì»¬ëŸ¼ í™•ì¸\n",
    "    required_train_cols = ['title', 'full_text', 'generated']\n",
    "    required_test_cols = ['title', 'full_text']\n",
    "    \n",
    "    missing_train_cols = [col for col in required_train_cols if col not in train.columns]\n",
    "    missing_test_cols = [col for col in required_test_cols if col not in test.columns]\n",
    "    \n",
    "    if missing_train_cols:\n",
    "        print(f\"âŒ Train ë°ì´í„°ì—ì„œ ëˆ„ë½ëœ ì»¬ëŸ¼: {missing_train_cols}\")\n",
    "        print(f\"ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {train.columns.tolist()}\")\n",
    "        raise ValueError(\"í•„ìš”í•œ ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    if missing_test_cols:\n",
    "        print(f\"âŒ Test ë°ì´í„°ì—ì„œ ëˆ„ë½ëœ ì»¬ëŸ¼: {missing_test_cols}\")\n",
    "        print(f\"ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {test.columns.tolist()}\")\n",
    "        raise ValueError(\"í•„ìš”í•œ ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    # ë°ì´í„° ë¶„í• \n",
    "    X = train[['title', 'full_text']]\n",
    "    y = train['generated']\n",
    "    \n",
    "    print(f\"âœ… í”¼ì²˜ ë°ì´í„° shape: {X.shape}\")\n",
    "    print(f\"âœ… íƒ€ê²Ÿ ë°ì´í„° shape: {y.shape}\")\n",
    "    \n",
    "    # Train-Validation ë¶„í• \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, stratify=y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… í›ˆë ¨ ì„¸íŠ¸: {X_train.shape}\")\n",
    "    print(f\"âœ… ê²€ì¦ ì„¸íŠ¸: {X_val.shape}\")\n",
    "    print(f\"âœ… í›ˆë ¨ ì„¸íŠ¸ íƒ€ê²Ÿ ë¹„ìœ¨: {y_train.mean():.3f}\")\n",
    "    print(f\"âœ… ê²€ì¦ ì„¸íŠ¸ íƒ€ê²Ÿ ë¹„ìœ¨: {y_val.mean():.3f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ë°ì´í„° ì „ì²˜ë¦¬ ì˜¤ë¥˜: {e}\")\n",
    "    print(\"ë°ì´í„° êµ¬ì¡°ë¥¼ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc44e7d",
   "metadata": {
    "id": "fbc44e7d"
   },
   "source": [
    "# ê°œì„ ëœ TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c9cafe",
   "metadata": {
    "id": "f3c9cafe"
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      6\u001b[39m vectorizer = FeatureUnion([\n\u001b[32m      7\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m, Pipeline([(\u001b[33m'\u001b[39m\u001b[33mselector\u001b[39m\u001b[33m'\u001b[39m, get_title),\n\u001b[32m      8\u001b[39m                         (\u001b[33m'\u001b[39m\u001b[33mtfidf\u001b[39m\u001b[33m'\u001b[39m, TfidfVectorizer(ngram_range=(\u001b[32m1\u001b[39m,\u001b[32m3\u001b[39m), max_features=\u001b[32m5000\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m                                                     min_df=\u001b[32m2\u001b[39m, max_df=\u001b[32m0.95\u001b[39m))])),\n\u001b[32m     13\u001b[39m ])\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# í”¼ì²˜ ë³€í™˜\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m X_train_vec = \u001b[43mvectorizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m X_val_vec = vectorizer.transform(X_val)\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mí”¼ì²˜ ì°¨ì›: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train_vec.shape[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    322\u001b[39m         return_tuple = (\n\u001b[32m    323\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    324\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    325\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\pipeline.py:1974\u001b[39m, in \u001b[36mFeatureUnion.fit_transform\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1971\u001b[39m             routed_params[name] = Bunch(transform={})\n\u001b[32m   1972\u001b[39m             routed_params[name].fit = params\n\u001b[32m-> \u001b[39m\u001b[32m1974\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parallel_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_fit_transform_one\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1975\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m results:\n\u001b[32m   1976\u001b[39m     \u001b[38;5;66;03m# All transformers are None\u001b[39;00m\n\u001b[32m   1977\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.zeros((X.shape[\u001b[32m0\u001b[39m], \u001b[32m0\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\pipeline.py:1996\u001b[39m, in \u001b[36mFeatureUnion._parallel_func\u001b[39m\u001b[34m(self, X, y, func, routed_params)\u001b[39m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_transformer_weights()\n\u001b[32m   1994\u001b[39m transformers = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m._iter())\n\u001b[32m-> \u001b[39m\u001b[32m1996\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1997\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1998\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2000\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2001\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFeatureUnion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtransformers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2005\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2006\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtransformers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2007\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1916\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1917\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1918\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1920\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1921\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1922\u001b[39m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1923\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1924\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1925\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1845\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1846\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1847\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1848\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1849\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:139\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m     config = {}\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config):\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\pipeline.py:1551\u001b[39m, in \u001b[36m_fit_transform_one\u001b[39m\u001b[34m(transformer, X, y, weight, message_clsname, message, params)\u001b[39m\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[32m   1550\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[33m\"\u001b[39m\u001b[33mfit_transform\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1551\u001b[39m         res = \u001b[43mtransformer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit_transform\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1552\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1553\u001b[39m         res = transformer.fit(X, y, **params.get(\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m, {})).transform(\n\u001b[32m   1554\u001b[39m             X, **params.get(\u001b[33m\"\u001b[39m\u001b[33mtransform\u001b[39m\u001b[33m\"\u001b[39m, {})\n\u001b[32m   1555\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\pipeline.py:730\u001b[39m, in \u001b[36mPipeline.fit_transform\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    724\u001b[39m last_step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    725\u001b[39m     step_idx=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) - \u001b[32m1\u001b[39m,\n\u001b[32m    726\u001b[39m     step_params=routed_params[\u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]],\n\u001b[32m    727\u001b[39m     all_params=params,\n\u001b[32m    728\u001b[39m )\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(last_step, \u001b[33m\"\u001b[39m\u001b[33mfit_transform\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m730\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlast_step\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m        \u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit_transform\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m last_step.fit(Xt, y, **last_step_params[\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m]).transform(\n\u001b[32m    735\u001b[39m         Xt, **last_step_params[\u001b[33m\"\u001b[39m\u001b[33mtransform\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    736\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2104\u001b[39m, in \u001b[36mTfidfVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   2097\u001b[39m \u001b[38;5;28mself\u001b[39m._check_params()\n\u001b[32m   2098\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf = TfidfTransformer(\n\u001b[32m   2099\u001b[39m     norm=\u001b[38;5;28mself\u001b[39m.norm,\n\u001b[32m   2100\u001b[39m     use_idf=\u001b[38;5;28mself\u001b[39m.use_idf,\n\u001b[32m   2101\u001b[39m     smooth_idf=\u001b[38;5;28mself\u001b[39m.smooth_idf,\n\u001b[32m   2102\u001b[39m     sublinear_tf=\u001b[38;5;28mself\u001b[39m.sublinear_tf,\n\u001b[32m   2103\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2104\u001b[39m X = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2105\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf.fit(X)\n\u001b[32m   2106\u001b[39m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[32m   2107\u001b[39m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1376\u001b[39m, in \u001b[36mCountVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   1368\u001b[39m             warnings.warn(\n\u001b[32m   1369\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mUpper case characters found in\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1370\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m vocabulary while \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlowercase\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1371\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m is True. These entries will not\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1372\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m be matched with any documents\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1373\u001b[39m             )\n\u001b[32m   1374\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1376\u001b[39m vocabulary, X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.binary:\n\u001b[32m   1379\u001b[39m     X.data.fill(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1265\u001b[39m, in \u001b[36mCountVectorizer._count_vocab\u001b[39m\u001b[34m(self, raw_documents, fixed_vocab)\u001b[39m\n\u001b[32m   1263\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m analyze(doc):\n\u001b[32m   1264\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1265\u001b[39m         feature_idx = \u001b[43mvocabulary\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   1266\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m feature_idx \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m feature_counter:\n\u001b[32m   1267\u001b[39m             feature_counter[feature_idx] = \u001b[32m1\u001b[39m\n",
      "\u001b[31mMemoryError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ğŸš€ ë©”ëª¨ë¦¬ ìµœì í™”ëœ TF-IDF ë²¡í„°í™” (ë©”ëª¨ë¦¬ ì—ëŸ¬ í•´ê²°)\n",
    "import gc\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import time\n",
    "\n",
    "print(\"âš¡ ë©”ëª¨ë¦¬ ì ˆì•½ ë²¡í„°í™” ì‹œì‘...\")\n",
    "start_time = time.time()\n",
    "\n",
    "get_title = FunctionTransformer(lambda x: x['title'], validate=False)\n",
    "get_text = FunctionTransformer(lambda x: x['full_text'], validate=False)\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•œ ëŒ€í­ ì¶•ì†Œëœ ë²¡í„°í™”\n",
    "vectorizer = FeatureUnion([\n",
    "    ('title', Pipeline([\n",
    "        ('selector', get_title),\n",
    "        ('tfidf', TfidfVectorizer(\n",
    "            ngram_range=(1,2), \n",
    "            max_features=800,       # 1200â†’800 (33% ê°ì†Œ)\n",
    "            min_df=8,               # 5â†’8 (ë” ì—„ê²©í•œ í•„í„°ë§)\n",
    "            max_df=0.8,             # 0.85â†’0.8 (ë” ì—„ê²©í•œ í•„í„°ë§)\n",
    "            dtype='float32',        # ë©”ëª¨ë¦¬ ì ˆì•½\n",
    "            lowercase=True,         # ì†ë„ í–¥ìƒ\n",
    "            stop_words=None         # ì†ë„ í–¥ìƒ\n",
    "        ))\n",
    "    ])),\n",
    "    ('full_text', Pipeline([\n",
    "        ('selector', get_text),\n",
    "        ('tfidf', TfidfVectorizer(\n",
    "            ngram_range=(1,2), \n",
    "            max_features=3000,      # 5000â†’3000 (40% ê°ì†Œ)\n",
    "            min_df=8,               # 5â†’8 (ë” ì—„ê²©í•œ í•„í„°ë§)\n",
    "            max_df=0.8,             # 0.85â†’0.8 (ë” ì—„ê²©í•œ í•„í„°ë§)\n",
    "            dtype='float32',        # ë©”ëª¨ë¦¬ ì ˆì•½\n",
    "            lowercase=True,         # ì†ë„ í–¥ìƒ\n",
    "            stop_words=None         # ì†ë„ í–¥ìƒ\n",
    "        ))\n",
    "    ]))\n",
    "], n_jobs=1)  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ë³‘ë ¬ì²˜ë¦¬ ë¹„í™œì„±í™”\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì‚¬ì „ ì •ë¦¬\n",
    "gc.collect()\n",
    "print(\"ğŸ§¹ ì‚¬ì „ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\")\n",
    "\n",
    "# í”¼ì²˜ ë³€í™˜ (ë©”ëª¨ë¦¬ ì ˆì•½ ëª¨ë“œ)\n",
    "print(\"ğŸ“Š Title & Text ë²¡í„°í™” ì¤‘... (ë©”ëª¨ë¦¬ ì ˆì•½ ëª¨ë“œ)\")\n",
    "try:\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    print(f\"âœ… í›ˆë ¨ ë°ì´í„° ë²¡í„°í™” ì™„ë£Œ: {X_train_vec.shape}\")\n",
    "    \n",
    "    # ì¤‘ê°„ ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "    gc.collect()\n",
    "    \n",
    "    X_val_vec = vectorizer.transform(X_val)\n",
    "    print(f\"âœ… ê²€ì¦ ë°ì´í„° ë²¡í„°í™” ì™„ë£Œ: {X_val_vec.shape}\")\n",
    "    \n",
    "    # ì‹œê°„ ì¸¡ì •\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"âœ… ì „ì²´ ë²¡í„°í™” ì™„ë£Œ! ì†Œìš”ì‹œê°„: {elapsed_time:.1f}ì´ˆ\")\n",
    "    print(f\"âœ… ì´ í”¼ì²˜ ì°¨ì›: {X_train_vec.shape[1]} (ë©”ëª¨ë¦¬ ìµœì í™”)\")\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •\n",
    "    memory_usage_mb = (X_train_vec.shape[0] * X_train_vec.shape[1] * 4) / (1024*1024)  # float32 = 4bytes\n",
    "    print(f\"âœ… ì˜ˆìƒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: ~{memory_usage_mb:.0f}MB\")\n",
    "    \n",
    "    # ìµœì¢… ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "    gc.collect()\n",
    "    print(\"ğŸ§¹ ë²¡í„°í™” í›„ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\")\n",
    "    \n",
    "except MemoryError as e:\n",
    "    print(f\"âŒ ë©”ëª¨ë¦¬ ë¶€ì¡±: {e}\")\n",
    "    print(\"ğŸ’¡ í•´ê²°ì±…:\")\n",
    "    print(\"1. ëŸ°íƒ€ì„ ì¬ì‹œì‘ í›„ ë‹¤ì‹œ ì‹œë„\")\n",
    "    print(\"2. ë” ì‘ì€ ìƒ˜í”Œë¡œ í…ŒìŠ¤íŠ¸\")\n",
    "    print(\"3. ë¡œì»¬ í™˜ê²½ì—ì„œ ì‹¤í–‰\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ë²¡í„°í™” ì˜¤ë¥˜: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf606fd",
   "metadata": {
    "id": "7bf606fd"
   },
   "source": [
    "# ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ (ë¹„êµìš©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488bae9f",
   "metadata": {
    "id": "488bae9f"
   },
   "outputs": [],
   "source": [
    "# ğŸš€ ë©”ëª¨ë¦¬ ìµœì í™”ëœ XGBoost (ë² ì´ìŠ¤ë¼ì¸)\n",
    "import gc\n",
    "\n",
    "print(\"ğŸ” ì‹œìŠ¤í…œ í™˜ê²½ í™•ì¸...\")\n",
    "\n",
    "# Colabì—ì„œ GPU ì‚¬ìš© í™•ì¸\n",
    "try:\n",
    "    import subprocess\n",
    "    gpu_info = subprocess.check_output([\"nvidia-smi\"], universal_newlines=True)\n",
    "    print(\"âœ… GPU ì‚¬ìš© ê°€ëŠ¥\")\n",
    "    use_gpu = True\n",
    "except:\n",
    "    print(\"âš ï¸ GPU ì‚¬ìš© ë¶ˆê°€ - CPU ëª¨ë“œë¡œ ì „í™˜\")\n",
    "    use_gpu = False\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì ˆì•½ ìµœì í™”ëœ ì„¤ì •\n",
    "if use_gpu:\n",
    "    xgb_baseline = XGBClassifier(\n",
    "        n_estimators=50,        # 80â†’50 (ë©”ëª¨ë¦¬ ì ˆì•½)\n",
    "        max_depth=3,            # 4â†’3 (ë©”ëª¨ë¦¬ ì ˆì•½)\n",
    "        learning_rate=0.15,     # 0.1â†’0.15 (ì ì€ íŠ¸ë¦¬ë¡œ ë³´ìƒ)\n",
    "        subsample=0.8,          # ë©”ëª¨ë¦¬ ì ˆì•½\n",
    "        colsample_bytree=0.8,   # ë©”ëª¨ë¦¬ ì ˆì•½\n",
    "        random_state=42,\n",
    "        tree_method='gpu_hist',\n",
    "        device='cuda:0',        \n",
    "        max_bin=64,             # 128â†’64 (ë” ë§ì€ ë©”ëª¨ë¦¬ ì ˆì•½)\n",
    "        n_jobs=1                # GPU ì‚¬ìš© ì‹œ 1ë¡œ ì„¤ì •\n",
    "    )\n",
    "    print(\"ğŸš€ GPU ê°€ì† ëª¨ë“œ (ë©”ëª¨ë¦¬ ìµœì í™”)\")\n",
    "else:\n",
    "    xgb_baseline = XGBClassifier(\n",
    "        n_estimators=50,        # ë©”ëª¨ë¦¬ ì ˆì•½\n",
    "        max_depth=3,            # ë©”ëª¨ë¦¬ ì ˆì•½\n",
    "        learning_rate=0.15,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=2                # CPUì—ì„œëŠ” ì ë‹¹í•œ ë³‘ë ¬ì²˜ë¦¬\n",
    "    )\n",
    "    print(\"ğŸ–¥ï¸ CPU ëª¨ë“œ (ë©”ëª¨ë¦¬ ìµœì í™”)\")\n",
    "\n",
    "print(\"\\nğŸš€ ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n",
    "print(f\"í›ˆë ¨ ë°ì´í„° í¬ê¸°: {X_train_vec.shape}\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì‚¬ì „ ì •ë¦¬\n",
    "gc.collect()\n",
    "\n",
    "try:\n",
    "    # ëª¨ë¸ í•™ìŠµ\n",
    "    xgb_baseline.fit(X_train_vec, y_train)\n",
    "    print(\"âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n",
    "    \n",
    "    # ì˜ˆì¸¡ ë° í‰ê°€\n",
    "    val_probs_baseline = xgb_baseline.predict_proba(X_val_vec)[:, 1]\n",
    "    auc_baseline = roc_auc_score(y_val, val_probs_baseline)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥:\")\n",
    "    print(f\"Validation AUC: {auc_baseline:.4f}\")\n",
    "    \n",
    "    # GPU ë©”ëª¨ë¦¬ ì •ë³´ (ê°€ëŠ¥í•œ ê²½ìš°)\n",
    "    if use_gpu:\n",
    "        try:\n",
    "            gpu_mem = subprocess.check_output([\"nvidia-smi\", \"--query-gpu=memory.used,memory.total\", \"--format=csv,noheader,nounits\"], universal_newlines=True)\n",
    "            print(f\"GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {gpu_mem.strip()}\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print(\"âœ… ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ ì™„ë£Œ!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ëª¨ë¸ í•™ìŠµ ì˜¤ë¥˜: {e}\")\n",
    "    print(\"ğŸ’¡ í•´ê²°ì±…:\")\n",
    "    print(\"1. ëŸ°íƒ€ì„ ì¬ì‹œì‘\")\n",
    "    print(\"2. ë” ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ í…ŒìŠ¤íŠ¸\")\n",
    "    print(\"3. CPU ëª¨ë“œë¡œ ì „í™˜\")\n",
    "    raise\n",
    "finally:\n",
    "    # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "    gc.collect()\n",
    "    print(\"ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab40749",
   "metadata": {
    "id": "eab40749"
   },
   "source": [
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ëœ ëª¨ë¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096575f3",
   "metadata": {
    "id": "096575f3"
   },
   "outputs": [],
   "source": [
    "# ğŸš€ ë©”ëª¨ë¦¬ ì ˆì•½í˜• í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ëª¨ë¸\n",
    "import gc\n",
    "\n",
    "print(\"ğŸ”§ ê°œì„ ëœ ëª¨ë¸ ì„¤ì • ì¤‘...\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì‚¬ì „ ì •ë¦¬\n",
    "gc.collect()\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì ˆì•½ + ì„±ëŠ¥ ìµœì í™” ì„¤ì •\n",
    "if use_gpu:\n",
    "    xgb_improved = XGBClassifier(\n",
    "        n_estimators=80,            # 150â†’80 (ë©”ëª¨ë¦¬ ì ˆì•½)\n",
    "        max_depth=4,                # 5â†’4 (ë©”ëª¨ë¦¬ ì ˆì•½)\n",
    "        learning_rate=0.12,         # 0.1â†’0.12 (ì ì€ íŠ¸ë¦¬ë¡œ ë³´ìƒ)\n",
    "        subsample=0.85,             # 0.8â†’0.85 (ì•½ê°„ ì¦ê°€)\n",
    "        colsample_bytree=0.85,      # 0.8â†’0.85 (ì•½ê°„ ì¦ê°€)\n",
    "        reg_alpha=0.1,              \n",
    "        reg_lambda=1.0,             \n",
    "        random_state=42,\n",
    "        tree_method='gpu_hist',     # GPU ê°€ì†\n",
    "        device='cuda:0',            # ìƒˆë¡œìš´ GPU ì„¤ì •\n",
    "        max_bin=64,                 # ë©”ëª¨ë¦¬ ì ˆì•½\n",
    "        n_jobs=1                    # GPU ì‚¬ìš© ì‹œ 1ë¡œ ì„¤ì •\n",
    "    )\n",
    "    print(\"ğŸš€ GPU ê°€ì† ëª¨ë“œ (ì„±ëŠ¥ ìµœì í™”)\")\n",
    "else:\n",
    "    xgb_improved = XGBClassifier(\n",
    "        n_estimators=80,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.12,\n",
    "        subsample=0.85,\n",
    "        colsample_bytree=0.85,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=42,\n",
    "        n_jobs=2                    # CPUì—ì„œëŠ” ì ë‹¹í•œ ë³‘ë ¬ì²˜ë¦¬\n",
    "    )\n",
    "    print(\"ğŸ–¥ï¸ CPU ëª¨ë“œ (ì„±ëŠ¥ ìµœì í™”)\")\n",
    "\n",
    "try:\n",
    "    print(\"\\nğŸš€ ê°œì„ ëœ ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n",
    "    xgb_improved.fit(X_train_vec, y_train)\n",
    "    print(\"âœ… ê°œì„  ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n",
    "    \n",
    "    # ì˜ˆì¸¡ ë° í‰ê°€\n",
    "    val_probs_improved = xgb_improved.predict_proba(X_val_vec)[:, 1]\n",
    "    auc_improved = roc_auc_score(y_val, val_probs_improved)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š ê°œì„ ëœ ëª¨ë¸ ì„±ëŠ¥:\")\n",
    "    print(f\"Validation AUC: {auc_improved:.4f}\")\n",
    "    print(f\"ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„ í–¥ìƒ: {auc_improved - auc_baseline:.4f}\")\n",
    "    \n",
    "    if auc_improved > auc_baseline:\n",
    "        print(\"ğŸ‰ ì„±ëŠ¥ í–¥ìƒ ë‹¬ì„±!\")\n",
    "    else:\n",
    "        print(\"ğŸ¤” ë² ì´ìŠ¤ë¼ì¸ê³¼ ìœ ì‚¬í•œ ì„±ëŠ¥\")\n",
    "    \n",
    "    print(\"âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì™„ë£Œ!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ê°œì„  ëª¨ë¸ í•™ìŠµ ì˜¤ë¥˜: {e}\")\n",
    "    print(\"ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "    # ì˜¤ë¥˜ ì‹œ ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ ì‚¬ìš©\n",
    "    xgb_improved = xgb_baseline\n",
    "    val_probs_improved = val_probs_baseline\n",
    "    auc_improved = auc_baseline\n",
    "    \n",
    "finally:\n",
    "    # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "    gc.collect()\n",
    "    print(\"ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a59ffbb",
   "metadata": {
    "id": "2a59ffbb"
   },
   "source": [
    "# êµì°¨ ê²€ì¦ìœ¼ë¡œ ì•ˆì •ì„± í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c799073",
   "metadata": {
    "id": "9c799073"
   },
   "outputs": [],
   "source": [
    "# ğŸ”„ êµì°¨ ê²€ì¦ (ë©”ëª¨ë¦¬ ì ˆì•½ ëª¨ë“œ)\n",
    "print(\"ğŸ¤” êµì°¨ ê²€ì¦ì„ ì‹¤í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?\")\n",
    "print(\"âš ï¸ ì£¼ì˜: êµì°¨ ê²€ì¦ì€ ë©”ëª¨ë¦¬ë¥¼ ë§ì´ ì‚¬ìš©í•˜ê³  ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤.\")\n",
    "\n",
    "# ê°„ë‹¨í•œ êµì°¨ ê²€ì¦ (ë©”ëª¨ë¦¬ ì ˆì•½)\n",
    "run_cv = True  # Falseë¡œ ì„¤ì •í•˜ë©´ êµì°¨ ê²€ì¦ ê±´ë„ˆë›°ê¸°\n",
    "\n",
    "if run_cv:\n",
    "    print(\"\\nğŸ”„ 3-fold êµì°¨ ê²€ì¦ ì‹œì‘... (ë©”ëª¨ë¦¬ ì ˆì•½)\")\n",
    "    \n",
    "    try:\n",
    "        from sklearn.model_selection import StratifiedKFold\n",
    "        \n",
    "        # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ 3-foldë¡œ ì¶•ì†Œ\n",
    "        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        \n",
    "        # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "        gc.collect()\n",
    "        \n",
    "        # êµì°¨ ê²€ì¦ ì‹¤í–‰ (ë‹¨ì¼ ìŠ¤ë ˆë“œë¡œ ë©”ëª¨ë¦¬ ì ˆì•½)\n",
    "        print(\"ğŸ“Š êµì°¨ ê²€ì¦ ì§„í–‰ ì¤‘...\")\n",
    "        cv_scores = cross_val_score(\n",
    "            xgb_improved, X_train_vec, y_train,  # ì „ì²´ê°€ ì•„ë‹Œ í›ˆë ¨ ë°ì´í„°ë§Œ ì‚¬ìš©\n",
    "            cv=cv, scoring='roc_auc', n_jobs=1   # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ n_jobs=1\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nğŸ“Š êµì°¨ ê²€ì¦ ê²°ê³¼:\")\n",
    "        print(f\"CV AUC: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}\")\n",
    "        print(f\"ê°œë³„ fold ì ìˆ˜: {cv_scores}\")\n",
    "        \n",
    "        # ê²€ì¦ ì•ˆì •ì„± í‰ê°€\n",
    "        if cv_scores.std() < 0.01:\n",
    "            print(\"âœ… ëª¨ë¸ì´ ì•ˆì •ì ì…ë‹ˆë‹¤ (ë‚®ì€ í‘œì¤€í¸ì°¨)\")\n",
    "        elif cv_scores.std() < 0.02:\n",
    "            print(\"ğŸ¤” ëª¨ë¸ì´ ì ë‹¹íˆ ì•ˆì •ì ì…ë‹ˆë‹¤\")\n",
    "        else:\n",
    "            print(\"âš ï¸ ëª¨ë¸ ì•ˆì •ì„±ì´ ë‚®ìŠµë‹ˆë‹¤ (ë†’ì€ í‘œì¤€í¸ì°¨)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ êµì°¨ ê²€ì¦ ì˜¤ë¥˜: {e}\")\n",
    "        print(\"êµì°¨ ê²€ì¦ì„ ê±´ë„ˆë›°ê³  ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.\")\n",
    "        cv_scores = [auc_improved]  # ë‹¨ì¼ ê²€ì¦ ì ìˆ˜ ì‚¬ìš©\n",
    "    \n",
    "    finally:\n",
    "        gc.collect()\n",
    "        print(\"ğŸ§¹ êµì°¨ ê²€ì¦ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\")\n",
    "        \n",
    "else:\n",
    "    print(\"â­ï¸ êµì°¨ ê²€ì¦ ê±´ë„ˆëœ€ (ë©”ëª¨ë¦¬ ì ˆì•½)\")\n",
    "    cv_scores = [auc_improved]  # ë‹¨ì¼ ê²€ì¦ ì ìˆ˜ ì‚¬ìš©\n",
    "\n",
    "print(f\"\\nğŸ“Š ìµœì¢… ëª¨ë¸ ì•ˆì •ì„±: {np.mean(cv_scores):.4f}\")\n",
    "print(\"âœ… êµì°¨ ê²€ì¦ ë‹¨ê³„ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3e8bfd",
   "metadata": {
    "id": "6e3e8bfd"
   },
   "source": [
    "# ì„±ëŠ¥ ë¹„êµ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8132af",
   "metadata": {
    "id": "3e8132af"
   },
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ\n",
    "models = ['ë² ì´ìŠ¤ë¼ì¸', 'ê°œì„ ëœ ëª¨ë¸']\n",
    "scores = [auc_baseline, auc_improved]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars = plt.bar(models, scores, color=['lightblue', 'orange'], alpha=0.7)\n",
    "plt.ylim(0.85, max(scores) + 0.01)\n",
    "plt.ylabel('Validation AUC')\n",
    "plt.title('ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# ì ìˆ˜ í‘œì‹œ\n",
    "for bar, score in zip(bars, scores):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
    "             f'{score:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72aec57",
   "metadata": {
    "id": "e72aec57"
   },
   "source": [
    "# Inference (ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì‚¬ìš©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f46531",
   "metadata": {
    "id": "25f46531"
   },
   "outputs": [],
   "source": [
    "# ìµœì¢… ëª¨ë¸ë¡œ ì „ì²´ ë°ì´í„° ì¬í›ˆë ¨\n",
    "print(\"ğŸ† ìµœì¢… ëª¨ë¸ ì¤€ë¹„ ì¤‘...\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•œ ìµœì¢… ëª¨ë¸ ì„¤ì •\n",
    "if use_gpu:\n",
    "    final_model = XGBClassifier(\n",
    "        n_estimators=100,           # 200â†’100 (ë©”ëª¨ë¦¬ ì ˆì•½)\n",
    "        max_depth=5,                # 6â†’5 (ë©”ëª¨ë¦¬ ì ˆì•½) \n",
    "        learning_rate=0.12,         # 0.1â†’0.12 (ì ì€ íŠ¸ë¦¬ë¡œ ë³´ìƒ)\n",
    "        subsample=0.85,\n",
    "        colsample_bytree=0.85,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=42,\n",
    "        tree_method='gpu_hist',     # GPU ê°€ì†\n",
    "        device='cuda:0',\n",
    "        max_bin=64,                 # ë©”ëª¨ë¦¬ ì ˆì•½\n",
    "        n_jobs=1\n",
    "    )\n",
    "    print(\"ğŸš€ GPU ê°€ì† ìµœì¢… ëª¨ë¸\")\n",
    "else:\n",
    "    final_model = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.12,\n",
    "        subsample=0.85,\n",
    "        colsample_bytree=0.85,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=42,\n",
    "        n_jobs=2                    # CPUì—ì„œëŠ” ì ë‹¹í•œ ë³‘ë ¬ì²˜ë¦¬\n",
    "    )\n",
    "    print(\"ğŸ–¥ï¸ CPU ìµœì¢… ëª¨ë¸\")\n",
    "\n",
    "try:\n",
    "    print(\"\\nğŸš€ ì „ì²´ í›ˆë ¨ ë°ì´í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ...\")\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "    gc.collect()\n",
    "    \n",
    "    # ì „ì²´ í›ˆë ¨ ë°ì´í„°ë¡œ ë²¡í„°í™” (ë©”ëª¨ë¦¬ ì ˆì•½ ëª¨ë“œ)\n",
    "    print(\"ğŸ“Š ì „ì²´ ë°ì´í„° ë²¡í„°í™” ì¤‘...\")\n",
    "    X_full_vec = vectorizer.fit_transform(X)\n",
    "    print(f\"âœ… ì „ì²´ ë°ì´í„° ë²¡í„°í™” ì™„ë£Œ: {X_full_vec.shape}\")\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ì¤‘ê°„ ì •ë¦¬\n",
    "    gc.collect()\n",
    "    \n",
    "    # ìµœì¢… ëª¨ë¸ í•™ìŠµ\n",
    "    final_model.fit(X_full_vec, y)\n",
    "    print(\"âœ… ìµœì¢… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬\n",
    "    print(\"\\nğŸ”® í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ì¤‘...\")\n",
    "    \n",
    "    # ì»¬ëŸ¼ëª… í†µì¼ (ì•ˆì „í•œ ë°©ì‹)\n",
    "    if 'paragraph_text' in test.columns:\n",
    "        test = test.rename(columns={'paragraph_text': 'full_text'})\n",
    "        print(\"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ì»¬ëŸ¼ëª… í†µì¼ ì™„ë£Œ\")\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„\n",
    "    X_test = test[['title', 'full_text']].copy()\n",
    "    print(f\"ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {X_test.shape}\")\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "    gc.collect()\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë²¡í„°í™”\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    print(f\"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ë²¡í„°í™” ì™„ë£Œ: {X_test_vec.shape}\")\n",
    "    \n",
    "    # ì˜ˆì¸¡ ì‹¤í–‰\n",
    "    probs = final_model.predict_proba(X_test_vec)[:, 1]\n",
    "    \n",
    "    print(f\"\\nğŸ¯ ì˜ˆì¸¡ ì™„ë£Œ!\")\n",
    "    print(f\"âœ… ì˜ˆì¸¡ê°’ ë²”ìœ„: [{probs.min():.3f}, {probs.max():.3f}]\")\n",
    "    print(f\"âœ… ì˜ˆì¸¡ê°’ í‰ê· : {probs.mean():.3f}\")\n",
    "    print(f\"ğŸ§  í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {get_memory_usage():.1f}MB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ìµœì¢… ì˜ˆì¸¡ ì˜¤ë¥˜: {e}\")\n",
    "    print(\"ğŸ’¡ í•´ê²°ì±…:\")\n",
    "    print(\"1. ëŸ°íƒ€ì„ ì¬ì‹œì‘ í›„ ë‹¤ì‹œ ì‹œë„\")\n",
    "    print(\"2. ë” ì‘ì€ ëª¨ë¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©\")\n",
    "    print(\"3. ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë‚˜ëˆ„ì–´ ì˜ˆì¸¡\")\n",
    "    raise\n",
    "    \n",
    "finally:\n",
    "    # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "    gc.collect()\n",
    "    print(\"ğŸ§¹ ì˜ˆì¸¡ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722ab174",
   "metadata": {
    "id": "722ab174"
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75066df6",
   "metadata": {
    "id": "75066df6"
   },
   "outputs": [],
   "source": [
    "# Submission íŒŒì¼ ìƒì„±\n",
    "try:\n",
    "    # sample_submission íŒŒì¼ ì½ê¸° (ì•ˆì „í•˜ê²Œ)\n",
    "    if 'sample_submission' not in locals():\n",
    "        sample_submission = read_csv_robust('sample_submission.csv')\n",
    "    \n",
    "    print(f\"Sample submission shape: {sample_submission.shape}\")\n",
    "    print(f\"Sample submission ì»¬ëŸ¼: {sample_submission.columns.tolist()}\")\n",
    "    \n",
    "    # ì˜ˆì¸¡ê°’ í• ë‹¹\n",
    "    sample_submission['generated'] = probs\n",
    "    \n",
    "    print(f\"ì˜ˆì¸¡ê°’ í†µê³„:\")\n",
    "    print(f\"- ìµœì†Ÿê°’: {probs.min():.4f}\")\n",
    "    print(f\"- ìµœëŒ“ê°’: {probs.max():.4f}\")\n",
    "    print(f\"- í‰ê· ê°’: {probs.mean():.4f}\")\n",
    "    print(f\"- í‘œì¤€í¸ì°¨: {probs.std():.4f}\")\n",
    "    \n",
    "    # íŒŒì¼ ì €ì¥\n",
    "    output_filename = 'improved_submission.csv'\n",
    "    sample_submission.to_csv(output_filename, index=False)\n",
    "    print(f\"âœ… ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_filename}\")\n",
    "    \n",
    "    # íŒŒì¼ ë‹¤ìš´ë¡œë“œ (ì½”ë©ì—ì„œ)\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        files.download(output_filename)\n",
    "        print(f\"âœ… íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {output_filename}\")\n",
    "    except ImportError:\n",
    "        print(\"ë¡œì»¬ í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. íŒŒì¼ì´ í˜„ì¬ ë””ë ‰í„°ë¦¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    # ì„±ëŠ¥ ê°œì„  ìš”ì•½\n",
    "    print(f\"\\n=== ğŸ¯ ì„±ëŠ¥ ê°œì„  ìš”ì•½ ===\")\n",
    "    if 'auc_baseline' in locals() and 'auc_improved' in locals():\n",
    "        print(f\"ë² ì´ìŠ¤ë¼ì¸ AUC: {auc_baseline:.4f}\")\n",
    "        print(f\"ê°œì„ ëœ AUC: {auc_improved:.4f}\")\n",
    "        print(f\"ê°œì„ í­: +{auc_improved - auc_baseline:.4f}\")\n",
    "    if 'cv_scores' in locals():\n",
    "        print(f\"êµì°¨ê²€ì¦ AUC: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}\")\n",
    "    \n",
    "    print(f\"\\nğŸ”¥ ìµœì¢… ëª¨ë¸ ì˜ˆì¸¡ ì™„ë£Œ!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì œì¶œ íŒŒì¼ ìƒì„± ì˜¤ë¥˜: {e}\")\n",
    "    print(\"ì´ì „ ë‹¨ê³„ë“¤ì´ ëª¨ë‘ ì„±ê³µì ìœ¼ë¡œ ì‹¤í–‰ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
