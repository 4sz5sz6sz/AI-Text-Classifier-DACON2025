{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e74b97e",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/4sz5sz6sz/AI-Text-Classifier-DACON2025/blob/main/improved_tfidf_xgboost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e16bb3",
   "metadata": {
    "id": "f3e16bb3"
   },
   "source": [
    "# TF-IDF + XGBoost 개선 실험\n",
    "\n",
    "**개선 사항:**\n",
    "- 하이퍼파라미터 튜닝 추가\n",
    "- 교차 검증 적용\n",
    "- 피처 개수 조정\n",
    "- 성능 비교 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b01a364",
   "metadata": {},
   "source": [
    "## 🚀 Google Colab 실행 가이드\n",
    "\n",
    "## 파일 업로드 방법\n",
    "\n",
    "**대용량 파일(500MB+)**: Google Drive 공개 링크 사용 (아래 코드 참고)\n",
    "**소용량 파일**: 직접 업로드\n",
    "\n",
    "**주의사항:**\n",
    "- 파일 업로드는 한 번만 하면 됩니다\n",
    "- 런타임이 재시작되면 다시 업로드해야 합니다\n",
    "- 인코딩 오류가 발생하면 자동으로 여러 방식을 시도합니다\n",
    "\n",
    "**실행 순서:**\n",
    "1. 모든 셀을 순서대로 실행\n",
    "2. 파일 업로드가 요청되면 데이터 파일들을 업로드\n",
    "3. 최종 결과는 자동으로 다운로드됩니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d8caaf",
   "metadata": {
    "id": "69d8caaf"
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcbb6404",
   "metadata": {
    "id": "dcbb6404"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b143f580",
   "metadata": {
    "id": "b143f580"
   },
   "source": [
    "# Data Load & Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf0105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔗 대용량 파일 자동 다운로드\n",
    "import gdown\n",
    "import os\n",
    "\n",
    "# 파일 ID 설정 (Google Drive 공개 링크에서 추출)\n",
    "TRAIN_FILE_ID = \"1teA9GmYlIsutaDLWvCCsLeh7833t-TC_\"\n",
    "TEST_FILE_ID = \"1bGC_YWtNUOroHARfmzCjrL8oPcvb7Tpw\"\n",
    "SAMPLE_FILE_ID = \"1ebrHVj-CtM-7aEz4OqP-bCHlazle-PKM\"\n",
    "\n",
    "def download_from_drive(file_id, filename):\n",
    "    if file_id and not os.path.exists(filename):\n",
    "        url = f'https://drive.google.com/uc?id={file_id}'\n",
    "        print(f\"📥 {filename} 다운로드 중...\")\n",
    "        gdown.download(url, filename, quiet=False)\n",
    "        print(f\"✅ {filename} 다운로드 완료!\")\n",
    "    elif os.path.exists(filename):\n",
    "        print(f\"✅ {filename} 이미 존재함\")\n",
    "\n",
    "# 자동 다운로드 실행\n",
    "download_from_drive(TRAIN_FILE_ID, 'train.csv')\n",
    "download_from_drive(TEST_FILE_ID, 'test.csv')\n",
    "download_from_drive(SAMPLE_FILE_ID, 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4zGT3sBgtRHM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4zGT3sBgtRHM",
    "outputId": "a579bdde-4e89-4e86-9543-32e8c8caa07a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== train.csv 읽기 ===\n",
      "'utf-8-sig' 인코딩으로 시도 중...\n",
      "'utf-8-sig' 인코딩에서 다른 오류: Error tokenizing data. C error: out of memory\n",
      "'utf-8' 인코딩으로 시도 중...\n",
      "'utf-8-sig' 인코딩에서 다른 오류: Error tokenizing data. C error: out of memory\n",
      "'utf-8' 인코딩으로 시도 중...\n",
      "성공! 'utf-8' 인코딩으로 파일을 읽었습니다.\n",
      "Train 데이터 shape: (97172, 3)\n",
      "\n",
      "=== test.csv 읽기 ===\n",
      "'utf-8-sig' 인코딩으로 시도 중...\n",
      "성공! 'utf-8-sig' 인코딩으로 파일을 읽었습니다.\n",
      "Test 데이터 shape: (1962, 4)\n",
      "성공! 'utf-8' 인코딩으로 파일을 읽었습니다.\n",
      "Train 데이터 shape: (97172, 3)\n",
      "\n",
      "=== test.csv 읽기 ===\n",
      "'utf-8-sig' 인코딩으로 시도 중...\n",
      "성공! 'utf-8-sig' 인코딩으로 파일을 읽었습니다.\n",
      "Test 데이터 shape: (1962, 4)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import psutil\n",
    "\n",
    "# 메모리 사용량 확인 함수\n",
    "def get_memory_usage():\n",
    "    \"\"\"현재 메모리 사용량 확인\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "    return memory_mb\n",
    "\n",
    "print(f\"🔍 시작 시 메모리 사용량: {get_memory_usage():.1f}MB\")\n",
    "\n",
    "# 메모리 효율적인 CSV 읽기 함수\n",
    "def read_csv_robust(file_path):\n",
    "    \"\"\"메모리 효율적이고 다양한 인코딩으로 CSV 파일을 읽는 함수\"\"\"\n",
    "    encodings = ['utf-8', 'utf-8-sig', 'cp949', 'euc-kr']\n",
    "    \n",
    "    print(f\"📂 {file_path} 로딩 중...\")\n",
    "    start_memory = get_memory_usage()\n",
    "    \n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            # 메모리 효율적 로딩 설정\n",
    "            df = pd.read_csv(\n",
    "                file_path, \n",
    "                encoding=encoding,\n",
    "                dtype={'id': 'int32'},  # ID는 int32로 충분\n",
    "                low_memory=False        # 타입 추론 개선\n",
    "            )\n",
    "            \n",
    "            end_memory = get_memory_usage()\n",
    "            memory_used = end_memory - start_memory\n",
    "            \n",
    "            print(f\"✅ {file_path} 읽기 성공\")\n",
    "            print(f\"   - 인코딩: {encoding}\")\n",
    "            print(f\"   - 메모리 사용: +{memory_used:.1f}MB\")\n",
    "            print(f\"   - 데이터 크기: {df.shape}\")\n",
    "            \n",
    "            return df\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    # 모든 인코딩 실패 시 에러 무시 모드\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding='utf-8', errors='ignore', low_memory=False)\n",
    "        end_memory = get_memory_usage()\n",
    "        memory_used = end_memory - start_memory\n",
    "        \n",
    "        print(f\"✅ {file_path} 읽기 성공 (에러 무시 모드)\")\n",
    "        print(f\"   - 메모리 사용: +{memory_used:.1f}MB\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"❌ {file_path} 읽기 실패: {e}\")\n",
    "        raise\n",
    "\n",
    "# 데이터 파일 읽기\n",
    "print(\"📁 데이터 파일 로딩 시작...\")\n",
    "gc.collect()  # 사전 메모리 정리\n",
    "\n",
    "try:\n",
    "    train = read_csv_robust('train.csv')\n",
    "    test = read_csv_robust('test.csv')\n",
    "    sample_submission = read_csv_robust('sample_submission.csv')\n",
    "    \n",
    "    print(f\"\\n📊 데이터 로딩 완료!\")\n",
    "    print(f\"✅ Train: {train.shape}\")\n",
    "    print(f\"✅ Test: {test.shape}\")\n",
    "    print(f\"✅ Sample submission: {sample_submission.shape}\")\n",
    "    print(f\"🧠 총 메모리 사용량: {get_memory_usage():.1f}MB\")\n",
    "    \n",
    "    # 데이터 타입 최적화 (메모리 절약)\n",
    "    for df_name, df in [('train', train), ('test', test)]:\n",
    "        if 'id' in df.columns:\n",
    "            df['id'] = df['id'].astype('int32')\n",
    "        print(f\"📝 {df_name} 데이터 타입 최적화 완료\")\n",
    "    \n",
    "    gc.collect()\n",
    "    print(\"🧹 메모리 정리 완료\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 데이터 로딩 중 오류: {e}\")\n",
    "    print(\"💡 해결책:\")\n",
    "    print(\"1. 파일이 올바르게 다운로드되었는지 확인\")\n",
    "    print(\"2. 런타임 재시작 후 다시 시도\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b411202",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "6b411202",
    "outputId": "cbee5210-f6fd-4775-e951-e87ade0ecadb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (97172, 3)\n",
      "Test shape: (1962, 4)\n",
      "Generated 비율: 0.082\n"
     ]
    }
   ],
   "source": [
    "#train = pd.read_csv('./train.csv', encoding='utf-8-sig')\n",
    "#test = pd.read_csv('./test.csv', encoding='utf-8-sig')\n",
    "\n",
    "# 데이터 확인\n",
    "try:\n",
    "    print(\"=== 데이터 기본 정보 ===\")\n",
    "    print(f\"Train shape: {train.shape}\")\n",
    "    print(f\"Test shape: {test.shape}\")\n",
    "    \n",
    "    # 컬럼 확인\n",
    "    print(f\"\\nTrain 컬럼: {train.columns.tolist()}\")\n",
    "    print(f\"Test 컬럼: {test.columns.tolist()}\")\n",
    "    \n",
    "    # target 변수 확인\n",
    "    if 'generated' in train.columns:\n",
    "        print(f\"\\nGenerated 비율: {train['generated'].mean():.3f}\")\n",
    "        print(f\"Generated 값 분포:\")\n",
    "        print(train['generated'].value_counts())\n",
    "    else:\n",
    "        print(\"\\n⚠️ 'generated' 컬럼을 찾을 수 없습니다.\")\n",
    "        print(\"사용 가능한 컬럼:\", train.columns.tolist())\n",
    "    \n",
    "    # 데이터 미리보기\n",
    "    print(\"\\n=== Train 데이터 미리보기 ===\")\n",
    "    print(train.head())\n",
    "    \n",
    "    print(\"\\n=== Test 데이터 미리보기 ===\")\n",
    "    print(test.head())\n",
    "    \n",
    "    # 결측값 확인\n",
    "    print(\"\\n=== 결측값 확인 ===\")\n",
    "    print(\"Train 결측값:\")\n",
    "    print(train.isnull().sum())\n",
    "    print(\"\\nTest 결측값:\")\n",
    "    print(test.isnull().sum())\n",
    "    \n",
    "except NameError:\n",
    "    print(\"❌ 데이터가 로드되지 않았습니다. 위의 셀을 먼저 실행해주세요.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 데이터 확인 중 오류: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9c6724",
   "metadata": {
    "id": "cf9c6724"
   },
   "outputs": [],
   "source": [
    "# 데이터 전처리 및 분할\n",
    "try:\n",
    "    # 테스트 데이터 컬럼명 통일 (필요한 경우)\n",
    "    if 'paragraph_text' in test.columns and 'full_text' not in test.columns:\n",
    "        test = test.rename(columns={'paragraph_text': 'full_text'})\n",
    "        print(\"✅ Test 데이터의 'paragraph_text' 컬럼을 'full_text'로 변경했습니다.\")\n",
    "    \n",
    "    # 필요한 컬럼 확인\n",
    "    required_train_cols = ['title', 'full_text', 'generated']\n",
    "    required_test_cols = ['title', 'full_text']\n",
    "    \n",
    "    missing_train_cols = [col for col in required_train_cols if col not in train.columns]\n",
    "    missing_test_cols = [col for col in required_test_cols if col not in test.columns]\n",
    "    \n",
    "    if missing_train_cols:\n",
    "        print(f\"❌ Train 데이터에서 누락된 컬럼: {missing_train_cols}\")\n",
    "        print(f\"사용 가능한 컬럼: {train.columns.tolist()}\")\n",
    "        raise ValueError(\"필요한 컬럼이 누락되었습니다.\")\n",
    "    \n",
    "    if missing_test_cols:\n",
    "        print(f\"❌ Test 데이터에서 누락된 컬럼: {missing_test_cols}\")\n",
    "        print(f\"사용 가능한 컬럼: {test.columns.tolist()}\")\n",
    "        raise ValueError(\"필요한 컬럼이 누락되었습니다.\")\n",
    "    \n",
    "    # 데이터 분할\n",
    "    X = train[['title', 'full_text']]\n",
    "    y = train['generated']\n",
    "    \n",
    "    print(f\"✅ 피처 데이터 shape: {X.shape}\")\n",
    "    print(f\"✅ 타겟 데이터 shape: {y.shape}\")\n",
    "    \n",
    "    # Train-Validation 분할\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, stratify=y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ 훈련 세트: {X_train.shape}\")\n",
    "    print(f\"✅ 검증 세트: {X_val.shape}\")\n",
    "    print(f\"✅ 훈련 세트 타겟 비율: {y_train.mean():.3f}\")\n",
    "    print(f\"✅ 검증 세트 타겟 비율: {y_val.mean():.3f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 데이터 전처리 오류: {e}\")\n",
    "    print(\"데이터 구조를 다시 확인해주세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc44e7d",
   "metadata": {
    "id": "fbc44e7d"
   },
   "source": [
    "# 개선된 TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c9cafe",
   "metadata": {
    "id": "f3c9cafe"
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      6\u001b[39m vectorizer = FeatureUnion([\n\u001b[32m      7\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m, Pipeline([(\u001b[33m'\u001b[39m\u001b[33mselector\u001b[39m\u001b[33m'\u001b[39m, get_title),\n\u001b[32m      8\u001b[39m                         (\u001b[33m'\u001b[39m\u001b[33mtfidf\u001b[39m\u001b[33m'\u001b[39m, TfidfVectorizer(ngram_range=(\u001b[32m1\u001b[39m,\u001b[32m3\u001b[39m), max_features=\u001b[32m5000\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m                                                     min_df=\u001b[32m2\u001b[39m, max_df=\u001b[32m0.95\u001b[39m))])),\n\u001b[32m     13\u001b[39m ])\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# 피처 변환\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m X_train_vec = \u001b[43mvectorizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m X_val_vec = vectorizer.transform(X_val)\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m피처 차원: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train_vec.shape[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    322\u001b[39m         return_tuple = (\n\u001b[32m    323\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    324\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    325\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\pipeline.py:1974\u001b[39m, in \u001b[36mFeatureUnion.fit_transform\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1971\u001b[39m             routed_params[name] = Bunch(transform={})\n\u001b[32m   1972\u001b[39m             routed_params[name].fit = params\n\u001b[32m-> \u001b[39m\u001b[32m1974\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parallel_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_fit_transform_one\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1975\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m results:\n\u001b[32m   1976\u001b[39m     \u001b[38;5;66;03m# All transformers are None\u001b[39;00m\n\u001b[32m   1977\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.zeros((X.shape[\u001b[32m0\u001b[39m], \u001b[32m0\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\pipeline.py:1996\u001b[39m, in \u001b[36mFeatureUnion._parallel_func\u001b[39m\u001b[34m(self, X, y, func, routed_params)\u001b[39m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_transformer_weights()\n\u001b[32m   1994\u001b[39m transformers = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m._iter())\n\u001b[32m-> \u001b[39m\u001b[32m1996\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1997\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1998\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2000\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2001\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFeatureUnion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtransformers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2005\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2006\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtransformers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2007\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1916\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1917\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1918\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1920\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1921\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1922\u001b[39m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1923\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1924\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1925\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1845\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1846\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1847\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1848\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1849\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:139\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m     config = {}\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config):\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\pipeline.py:1551\u001b[39m, in \u001b[36m_fit_transform_one\u001b[39m\u001b[34m(transformer, X, y, weight, message_clsname, message, params)\u001b[39m\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[32m   1550\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[33m\"\u001b[39m\u001b[33mfit_transform\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1551\u001b[39m         res = \u001b[43mtransformer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit_transform\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1552\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1553\u001b[39m         res = transformer.fit(X, y, **params.get(\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m, {})).transform(\n\u001b[32m   1554\u001b[39m             X, **params.get(\u001b[33m\"\u001b[39m\u001b[33mtransform\u001b[39m\u001b[33m\"\u001b[39m, {})\n\u001b[32m   1555\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\pipeline.py:730\u001b[39m, in \u001b[36mPipeline.fit_transform\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    724\u001b[39m last_step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    725\u001b[39m     step_idx=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) - \u001b[32m1\u001b[39m,\n\u001b[32m    726\u001b[39m     step_params=routed_params[\u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]],\n\u001b[32m    727\u001b[39m     all_params=params,\n\u001b[32m    728\u001b[39m )\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(last_step, \u001b[33m\"\u001b[39m\u001b[33mfit_transform\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m730\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlast_step\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m        \u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit_transform\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m last_step.fit(Xt, y, **last_step_params[\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m]).transform(\n\u001b[32m    735\u001b[39m         Xt, **last_step_params[\u001b[33m\"\u001b[39m\u001b[33mtransform\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    736\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2104\u001b[39m, in \u001b[36mTfidfVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   2097\u001b[39m \u001b[38;5;28mself\u001b[39m._check_params()\n\u001b[32m   2098\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf = TfidfTransformer(\n\u001b[32m   2099\u001b[39m     norm=\u001b[38;5;28mself\u001b[39m.norm,\n\u001b[32m   2100\u001b[39m     use_idf=\u001b[38;5;28mself\u001b[39m.use_idf,\n\u001b[32m   2101\u001b[39m     smooth_idf=\u001b[38;5;28mself\u001b[39m.smooth_idf,\n\u001b[32m   2102\u001b[39m     sublinear_tf=\u001b[38;5;28mself\u001b[39m.sublinear_tf,\n\u001b[32m   2103\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2104\u001b[39m X = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2105\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf.fit(X)\n\u001b[32m   2106\u001b[39m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[32m   2107\u001b[39m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1376\u001b[39m, in \u001b[36mCountVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   1368\u001b[39m             warnings.warn(\n\u001b[32m   1369\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mUpper case characters found in\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1370\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m vocabulary while \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlowercase\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1371\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m is True. These entries will not\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1372\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m be matched with any documents\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1373\u001b[39m             )\n\u001b[32m   1374\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1376\u001b[39m vocabulary, X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.binary:\n\u001b[32m   1379\u001b[39m     X.data.fill(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgms9\\miniconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1265\u001b[39m, in \u001b[36mCountVectorizer._count_vocab\u001b[39m\u001b[34m(self, raw_documents, fixed_vocab)\u001b[39m\n\u001b[32m   1263\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m analyze(doc):\n\u001b[32m   1264\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1265\u001b[39m         feature_idx = \u001b[43mvocabulary\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   1266\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m feature_idx \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m feature_counter:\n\u001b[32m   1267\u001b[39m             feature_counter[feature_idx] = \u001b[32m1\u001b[39m\n",
      "\u001b[31mMemoryError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 🚀 메모리 최적화된 TF-IDF 벡터화 (메모리 에러 해결)\n",
    "import gc\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import time\n",
    "\n",
    "print(\"⚡ 메모리 절약 벡터화 시작...\")\n",
    "start_time = time.time()\n",
    "\n",
    "get_title = FunctionTransformer(lambda x: x['title'], validate=False)\n",
    "get_text = FunctionTransformer(lambda x: x['full_text'], validate=False)\n",
    "\n",
    "# 메모리 절약을 위한 대폭 축소된 벡터화\n",
    "vectorizer = FeatureUnion([\n",
    "    ('title', Pipeline([\n",
    "        ('selector', get_title),\n",
    "        ('tfidf', TfidfVectorizer(\n",
    "            ngram_range=(1,2), \n",
    "            max_features=800,       # 1200→800 (33% 감소)\n",
    "            min_df=8,               # 5→8 (더 엄격한 필터링)\n",
    "            max_df=0.8,             # 0.85→0.8 (더 엄격한 필터링)\n",
    "            dtype='float32',        # 메모리 절약\n",
    "            lowercase=True,         # 속도 향상\n",
    "            stop_words=None         # 속도 향상\n",
    "        ))\n",
    "    ])),\n",
    "    ('full_text', Pipeline([\n",
    "        ('selector', get_text),\n",
    "        ('tfidf', TfidfVectorizer(\n",
    "            ngram_range=(1,2), \n",
    "            max_features=3000,      # 5000→3000 (40% 감소)\n",
    "            min_df=8,               # 5→8 (더 엄격한 필터링)\n",
    "            max_df=0.8,             # 0.85→0.8 (더 엄격한 필터링)\n",
    "            dtype='float32',        # 메모리 절약\n",
    "            lowercase=True,         # 속도 향상\n",
    "            stop_words=None         # 속도 향상\n",
    "        ))\n",
    "    ]))\n",
    "], n_jobs=1)  # 메모리 절약을 위해 병렬처리 비활성화\n",
    "\n",
    "# 메모리 사전 정리\n",
    "gc.collect()\n",
    "print(\"🧹 사전 메모리 정리 완료\")\n",
    "\n",
    "# 피처 변환 (메모리 절약 모드)\n",
    "print(\"📊 Title & Text 벡터화 중... (메모리 절약 모드)\")\n",
    "try:\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    print(f\"✅ 훈련 데이터 벡터화 완료: {X_train_vec.shape}\")\n",
    "    \n",
    "    # 중간 메모리 정리\n",
    "    gc.collect()\n",
    "    \n",
    "    X_val_vec = vectorizer.transform(X_val)\n",
    "    print(f\"✅ 검증 데이터 벡터화 완료: {X_val_vec.shape}\")\n",
    "    \n",
    "    # 시간 측정\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"✅ 전체 벡터화 완료! 소요시간: {elapsed_time:.1f}초\")\n",
    "    print(f\"✅ 총 피처 차원: {X_train_vec.shape[1]} (메모리 최적화)\")\n",
    "    \n",
    "    # 메모리 사용량 추정\n",
    "    memory_usage_mb = (X_train_vec.shape[0] * X_train_vec.shape[1] * 4) / (1024*1024)  # float32 = 4bytes\n",
    "    print(f\"✅ 예상 메모리 사용량: ~{memory_usage_mb:.0f}MB\")\n",
    "    \n",
    "    # 최종 메모리 정리\n",
    "    gc.collect()\n",
    "    print(\"🧹 벡터화 후 메모리 정리 완료\")\n",
    "    \n",
    "except MemoryError as e:\n",
    "    print(f\"❌ 메모리 부족: {e}\")\n",
    "    print(\"💡 해결책:\")\n",
    "    print(\"1. 런타임 재시작 후 다시 시도\")\n",
    "    print(\"2. 더 작은 샘플로 테스트\")\n",
    "    print(\"3. 로컬 환경에서 실행\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"❌ 벡터화 오류: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf606fd",
   "metadata": {
    "id": "7bf606fd"
   },
   "source": [
    "# 베이스라인 모델 (비교용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488bae9f",
   "metadata": {
    "id": "488bae9f"
   },
   "outputs": [],
   "source": [
    "# 🚀 메모리 최적화된 XGBoost (베이스라인)\n",
    "import gc\n",
    "\n",
    "print(\"🔍 시스템 환경 확인...\")\n",
    "\n",
    "# Colab에서 GPU 사용 확인\n",
    "try:\n",
    "    import subprocess\n",
    "    gpu_info = subprocess.check_output([\"nvidia-smi\"], universal_newlines=True)\n",
    "    print(\"✅ GPU 사용 가능\")\n",
    "    use_gpu = True\n",
    "except:\n",
    "    print(\"⚠️ GPU 사용 불가 - CPU 모드로 전환\")\n",
    "    use_gpu = False\n",
    "\n",
    "# 메모리 절약 최적화된 설정\n",
    "if use_gpu:\n",
    "    xgb_baseline = XGBClassifier(\n",
    "        n_estimators=50,        # 80→50 (메모리 절약)\n",
    "        max_depth=3,            # 4→3 (메모리 절약)\n",
    "        learning_rate=0.15,     # 0.1→0.15 (적은 트리로 보상)\n",
    "        subsample=0.8,          # 메모리 절약\n",
    "        colsample_bytree=0.8,   # 메모리 절약\n",
    "        random_state=42,\n",
    "        tree_method='gpu_hist',\n",
    "        device='cuda:0',        \n",
    "        max_bin=64,             # 128→64 (더 많은 메모리 절약)\n",
    "        n_jobs=1                # GPU 사용 시 1로 설정\n",
    "    )\n",
    "    print(\"🚀 GPU 가속 모드 (메모리 최적화)\")\n",
    "else:\n",
    "    xgb_baseline = XGBClassifier(\n",
    "        n_estimators=50,        # 메모리 절약\n",
    "        max_depth=3,            # 메모리 절약\n",
    "        learning_rate=0.15,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=2                # CPU에서는 적당한 병렬처리\n",
    "    )\n",
    "    print(\"🖥️ CPU 모드 (메모리 최적화)\")\n",
    "\n",
    "print(\"\\n🚀 베이스라인 모델 학습 시작...\")\n",
    "print(f\"훈련 데이터 크기: {X_train_vec.shape}\")\n",
    "\n",
    "# 메모리 사전 정리\n",
    "gc.collect()\n",
    "\n",
    "try:\n",
    "    # 모델 학습\n",
    "    xgb_baseline.fit(X_train_vec, y_train)\n",
    "    print(\"✅ 모델 학습 완료!\")\n",
    "    \n",
    "    # 예측 및 평가\n",
    "    val_probs_baseline = xgb_baseline.predict_proba(X_val_vec)[:, 1]\n",
    "    auc_baseline = roc_auc_score(y_val, val_probs_baseline)\n",
    "    \n",
    "    print(f\"\\n📊 베이스라인 성능:\")\n",
    "    print(f\"Validation AUC: {auc_baseline:.4f}\")\n",
    "    \n",
    "    # GPU 메모리 정보 (가능한 경우)\n",
    "    if use_gpu:\n",
    "        try:\n",
    "            gpu_mem = subprocess.check_output([\"nvidia-smi\", \"--query-gpu=memory.used,memory.total\", \"--format=csv,noheader,nounits\"], universal_newlines=True)\n",
    "            print(f\"GPU 메모리 사용량: {gpu_mem.strip()}\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print(\"✅ 베이스라인 모델 완료!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 모델 학습 오류: {e}\")\n",
    "    print(\"💡 해결책:\")\n",
    "    print(\"1. 런타임 재시작\")\n",
    "    print(\"2. 더 작은 데이터셋으로 테스트\")\n",
    "    print(\"3. CPU 모드로 전환\")\n",
    "    raise\n",
    "finally:\n",
    "    # 메모리 정리\n",
    "    gc.collect()\n",
    "    print(\"🧹 메모리 정리 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab40749",
   "metadata": {
    "id": "eab40749"
   },
   "source": [
    "# 하이퍼파라미터 튜닝된 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096575f3",
   "metadata": {
    "id": "096575f3"
   },
   "outputs": [],
   "source": [
    "# 🚀 메모리 절약형 하이퍼파라미터 튜닝 모델\n",
    "import gc\n",
    "\n",
    "print(\"🔧 개선된 모델 설정 중...\")\n",
    "\n",
    "# 메모리 사전 정리\n",
    "gc.collect()\n",
    "\n",
    "# 메모리 절약 + 성능 최적화 설정\n",
    "if use_gpu:\n",
    "    xgb_improved = XGBClassifier(\n",
    "        n_estimators=80,            # 150→80 (메모리 절약)\n",
    "        max_depth=4,                # 5→4 (메모리 절약)\n",
    "        learning_rate=0.12,         # 0.1→0.12 (적은 트리로 보상)\n",
    "        subsample=0.85,             # 0.8→0.85 (약간 증가)\n",
    "        colsample_bytree=0.85,      # 0.8→0.85 (약간 증가)\n",
    "        reg_alpha=0.1,              \n",
    "        reg_lambda=1.0,             \n",
    "        random_state=42,\n",
    "        tree_method='gpu_hist',     # GPU 가속\n",
    "        device='cuda:0',            # 새로운 GPU 설정\n",
    "        max_bin=64,                 # 메모리 절약\n",
    "        n_jobs=1                    # GPU 사용 시 1로 설정\n",
    "    )\n",
    "    print(\"🚀 GPU 가속 모드 (성능 최적화)\")\n",
    "else:\n",
    "    xgb_improved = XGBClassifier(\n",
    "        n_estimators=80,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.12,\n",
    "        subsample=0.85,\n",
    "        colsample_bytree=0.85,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=42,\n",
    "        n_jobs=2                    # CPU에서는 적당한 병렬처리\n",
    "    )\n",
    "    print(\"🖥️ CPU 모드 (성능 최적화)\")\n",
    "\n",
    "try:\n",
    "    print(\"\\n🚀 개선된 모델 학습 시작...\")\n",
    "    xgb_improved.fit(X_train_vec, y_train)\n",
    "    print(\"✅ 개선 모델 학습 완료!\")\n",
    "    \n",
    "    # 예측 및 평가\n",
    "    val_probs_improved = xgb_improved.predict_proba(X_val_vec)[:, 1]\n",
    "    auc_improved = roc_auc_score(y_val, val_probs_improved)\n",
    "    \n",
    "    print(f\"\\n📊 개선된 모델 성능:\")\n",
    "    print(f\"Validation AUC: {auc_improved:.4f}\")\n",
    "    print(f\"베이스라인 대비 향상: {auc_improved - auc_baseline:.4f}\")\n",
    "    \n",
    "    if auc_improved > auc_baseline:\n",
    "        print(\"🎉 성능 향상 달성!\")\n",
    "    else:\n",
    "        print(\"🤔 베이스라인과 유사한 성능\")\n",
    "    \n",
    "    print(\"✅ 하이퍼파라미터 튜닝 완료!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 개선 모델 학습 오류: {e}\")\n",
    "    print(\"베이스라인 모델을 사용합니다.\")\n",
    "    # 오류 시 베이스라인 모델 사용\n",
    "    xgb_improved = xgb_baseline\n",
    "    val_probs_improved = val_probs_baseline\n",
    "    auc_improved = auc_baseline\n",
    "    \n",
    "finally:\n",
    "    # 메모리 정리\n",
    "    gc.collect()\n",
    "    print(\"🧹 메모리 정리 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a59ffbb",
   "metadata": {
    "id": "2a59ffbb"
   },
   "source": [
    "# 교차 검증으로 안정성 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c799073",
   "metadata": {
    "id": "9c799073"
   },
   "outputs": [],
   "source": [
    "# 🔄 교차 검증 (메모리 절약 모드)\n",
    "print(\"🤔 교차 검증을 실행하시겠습니까?\")\n",
    "print(\"⚠️ 주의: 교차 검증은 메모리를 많이 사용하고 시간이 오래 걸립니다.\")\n",
    "\n",
    "# 간단한 교차 검증 (메모리 절약)\n",
    "run_cv = True  # False로 설정하면 교차 검증 건너뛰기\n",
    "\n",
    "if run_cv:\n",
    "    print(\"\\n🔄 3-fold 교차 검증 시작... (메모리 절약)\")\n",
    "    \n",
    "    try:\n",
    "        from sklearn.model_selection import StratifiedKFold\n",
    "        \n",
    "        # 메모리 절약을 위해 3-fold로 축소\n",
    "        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        \n",
    "        # 메모리 정리\n",
    "        gc.collect()\n",
    "        \n",
    "        # 교차 검증 실행 (단일 스레드로 메모리 절약)\n",
    "        print(\"📊 교차 검증 진행 중...\")\n",
    "        cv_scores = cross_val_score(\n",
    "            xgb_improved, X_train_vec, y_train,  # 전체가 아닌 훈련 데이터만 사용\n",
    "            cv=cv, scoring='roc_auc', n_jobs=1   # 메모리 절약을 위해 n_jobs=1\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n📊 교차 검증 결과:\")\n",
    "        print(f\"CV AUC: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "        print(f\"개별 fold 점수: {cv_scores}\")\n",
    "        \n",
    "        # 검증 안정성 평가\n",
    "        if cv_scores.std() < 0.01:\n",
    "            print(\"✅ 모델이 안정적입니다 (낮은 표준편차)\")\n",
    "        elif cv_scores.std() < 0.02:\n",
    "            print(\"🤔 모델이 적당히 안정적입니다\")\n",
    "        else:\n",
    "            print(\"⚠️ 모델 안정성이 낮습니다 (높은 표준편차)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 교차 검증 오류: {e}\")\n",
    "        print(\"교차 검증을 건너뛰고 계속 진행합니다.\")\n",
    "        cv_scores = [auc_improved]  # 단일 검증 점수 사용\n",
    "    \n",
    "    finally:\n",
    "        gc.collect()\n",
    "        print(\"🧹 교차 검증 후 메모리 정리 완료\")\n",
    "        \n",
    "else:\n",
    "    print(\"⏭️ 교차 검증 건너뜀 (메모리 절약)\")\n",
    "    cv_scores = [auc_improved]  # 단일 검증 점수 사용\n",
    "\n",
    "print(f\"\\n📊 최종 모델 안정성: {np.mean(cv_scores):.4f}\")\n",
    "print(\"✅ 교차 검증 단계 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3e8bfd",
   "metadata": {
    "id": "6e3e8bfd"
   },
   "source": [
    "# 성능 비교 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8132af",
   "metadata": {
    "id": "3e8132af"
   },
   "outputs": [],
   "source": [
    "# 모델 성능 비교\n",
    "models = ['베이스라인', '개선된 모델']\n",
    "scores = [auc_baseline, auc_improved]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars = plt.bar(models, scores, color=['lightblue', 'orange'], alpha=0.7)\n",
    "plt.ylim(0.85, max(scores) + 0.01)\n",
    "plt.ylabel('Validation AUC')\n",
    "plt.title('모델 성능 비교')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 점수 표시\n",
    "for bar, score in zip(bars, scores):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
    "             f'{score:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72aec57",
   "metadata": {
    "id": "e72aec57"
   },
   "source": [
    "# Inference (최고 성능 모델 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f46531",
   "metadata": {
    "id": "25f46531"
   },
   "outputs": [],
   "source": [
    "# 최종 모델로 전체 데이터 재훈련\n",
    "print(\"🏆 최종 모델 준비 중...\")\n",
    "\n",
    "# 메모리 절약을 위한 최종 모델 설정\n",
    "if use_gpu:\n",
    "    final_model = XGBClassifier(\n",
    "        n_estimators=100,           # 200→100 (메모리 절약)\n",
    "        max_depth=5,                # 6→5 (메모리 절약) \n",
    "        learning_rate=0.12,         # 0.1→0.12 (적은 트리로 보상)\n",
    "        subsample=0.85,\n",
    "        colsample_bytree=0.85,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=42,\n",
    "        tree_method='gpu_hist',     # GPU 가속\n",
    "        device='cuda:0',\n",
    "        max_bin=64,                 # 메모리 절약\n",
    "        n_jobs=1\n",
    "    )\n",
    "    print(\"🚀 GPU 가속 최종 모델\")\n",
    "else:\n",
    "    final_model = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.12,\n",
    "        subsample=0.85,\n",
    "        colsample_bytree=0.85,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=42,\n",
    "        n_jobs=2                    # CPU에서는 적당한 병렬처리\n",
    "    )\n",
    "    print(\"🖥️ CPU 최종 모델\")\n",
    "\n",
    "try:\n",
    "    print(\"\\n🚀 전체 훈련 데이터로 최종 모델 학습...\")\n",
    "    \n",
    "    # 메모리 정리\n",
    "    gc.collect()\n",
    "    \n",
    "    # 전체 훈련 데이터로 벡터화 (메모리 절약 모드)\n",
    "    print(\"📊 전체 데이터 벡터화 중...\")\n",
    "    X_full_vec = vectorizer.fit_transform(X)\n",
    "    print(f\"✅ 전체 데이터 벡터화 완료: {X_full_vec.shape}\")\n",
    "    \n",
    "    # 메모리 중간 정리\n",
    "    gc.collect()\n",
    "    \n",
    "    # 최종 모델 학습\n",
    "    final_model.fit(X_full_vec, y)\n",
    "    print(\"✅ 최종 모델 학습 완료!\")\n",
    "    \n",
    "    # 테스트 데이터 전처리\n",
    "    print(\"\\n🔮 테스트 데이터 예측 중...\")\n",
    "    \n",
    "    # 컬럼명 통일 (안전한 방식)\n",
    "    if 'paragraph_text' in test.columns:\n",
    "        test = test.rename(columns={'paragraph_text': 'full_text'})\n",
    "        print(\"✅ 테스트 데이터 컬럼명 통일 완료\")\n",
    "    \n",
    "    # 테스트 데이터 준비\n",
    "    X_test = test[['title', 'full_text']].copy()\n",
    "    print(f\"📊 테스트 데이터 준비 완료: {X_test.shape}\")\n",
    "    \n",
    "    # 메모리 정리\n",
    "    gc.collect()\n",
    "    \n",
    "    # 테스트 데이터 벡터화\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    print(f\"✅ 테스트 데이터 벡터화 완료: {X_test_vec.shape}\")\n",
    "    \n",
    "    # 예측 실행\n",
    "    probs = final_model.predict_proba(X_test_vec)[:, 1]\n",
    "    \n",
    "    print(f\"\\n🎯 예측 완료!\")\n",
    "    print(f\"✅ 예측값 범위: [{probs.min():.3f}, {probs.max():.3f}]\")\n",
    "    print(f\"✅ 예측값 평균: {probs.mean():.3f}\")\n",
    "    print(f\"🧠 현재 메모리 사용량: {get_memory_usage():.1f}MB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 최종 예측 오류: {e}\")\n",
    "    print(\"💡 해결책:\")\n",
    "    print(\"1. 런타임 재시작 후 다시 시도\")\n",
    "    print(\"2. 더 작은 모델 파라미터 사용\")\n",
    "    print(\"3. 배치 처리로 나누어 예측\")\n",
    "    raise\n",
    "    \n",
    "finally:\n",
    "    # 메모리 정리\n",
    "    gc.collect()\n",
    "    print(\"🧹 예측 후 메모리 정리 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722ab174",
   "metadata": {
    "id": "722ab174"
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75066df6",
   "metadata": {
    "id": "75066df6"
   },
   "outputs": [],
   "source": [
    "# Submission 파일 생성\n",
    "try:\n",
    "    # sample_submission 파일 읽기 (안전하게)\n",
    "    if 'sample_submission' not in locals():\n",
    "        sample_submission = read_csv_robust('sample_submission.csv')\n",
    "    \n",
    "    print(f\"Sample submission shape: {sample_submission.shape}\")\n",
    "    print(f\"Sample submission 컬럼: {sample_submission.columns.tolist()}\")\n",
    "    \n",
    "    # 예측값 할당\n",
    "    sample_submission['generated'] = probs\n",
    "    \n",
    "    print(f\"예측값 통계:\")\n",
    "    print(f\"- 최솟값: {probs.min():.4f}\")\n",
    "    print(f\"- 최댓값: {probs.max():.4f}\")\n",
    "    print(f\"- 평균값: {probs.mean():.4f}\")\n",
    "    print(f\"- 표준편차: {probs.std():.4f}\")\n",
    "    \n",
    "    # 파일 저장\n",
    "    output_filename = 'improved_submission.csv'\n",
    "    sample_submission.to_csv(output_filename, index=False)\n",
    "    print(f\"✅ 제출 파일 저장 완료: {output_filename}\")\n",
    "    \n",
    "    # 파일 다운로드 (코랩에서)\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        files.download(output_filename)\n",
    "        print(f\"✅ 파일 다운로드 시작: {output_filename}\")\n",
    "    except ImportError:\n",
    "        print(\"로컬 환경에서 실행 중입니다. 파일이 현재 디렉터리에 저장되었습니다.\")\n",
    "    \n",
    "    # 성능 개선 요약\n",
    "    print(f\"\\n=== 🎯 성능 개선 요약 ===\")\n",
    "    if 'auc_baseline' in locals() and 'auc_improved' in locals():\n",
    "        print(f\"베이스라인 AUC: {auc_baseline:.4f}\")\n",
    "        print(f\"개선된 AUC: {auc_improved:.4f}\")\n",
    "        print(f\"개선폭: +{auc_improved - auc_baseline:.4f}\")\n",
    "    if 'cv_scores' in locals():\n",
    "        print(f\"교차검증 AUC: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "    \n",
    "    print(f\"\\n🔥 최종 모델 예측 완료!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 제출 파일 생성 오류: {e}\")\n",
    "    print(\"이전 단계들이 모두 성공적으로 실행되었는지 확인해주세요.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
